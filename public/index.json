[
{
	"uri": "//localhost:1313/",
	"title": "AWS Aurora PostgreSQL",
	"tags": [],
	"description": "",
	"content": "Let start with AWS Aurora PostgreSQL Overall Welcome to the lab content for Amazon Aurora PostgreSQL . Here you will find a collection of workshops and other hands-on labs aimed to help you gain an understanding of the Amazon Aurora PostgreSQL features and capabilities.\nContent Introduction Preparation Fast Cloning Query Plan Management Cluster Cache Management Database Activity Streamning RDS Performance Insights Create Dataset and Autoscale Test Fault Tolerance Comparation RDS PostgreSQL and Aurora PostgreSQL Clean up "
},
{
	"uri": "//localhost:1313/2-preparation/2-1-createvpc/",
	"title": "Create a VPC",
	"tags": [],
	"description": "",
	"content": " Go to the VPC console and click Create VPC. For Resources to create, choose VPC and more.\nEnter a name for your VPC and select a CIDR block. The CIDR block is the range of IP addresses that will be available to your VPC. Make sure to choose a CIDR block that is large enough for your needs, but not so large that you waste IP addresses. Select values for Number of public subnets, Number of private subnets and NAT Gateway.\nReview your VPC resources, then click Create VPC "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "\nAmazon Aurora PostgreSQL is a fully managed, PostgreSQL–compatible, and ACID–compliant relational database engine that combines the speed, reliability, and manageability of Amazon Aurora with the simplicity and cost-effectiveness of open-source databases. Aurora PostgreSQL is a drop-in replacement for PostgreSQL and makes it simple and cost-effective to set up, operate, and scale your new and existing PostgreSQL deployments, thus freeing you to focus on your business and applications. To learn more about Aurora in general, see What is Amazon Aurora?.\nIn addition to the benefits of Aurora, Aurora PostgreSQL offers a convenient migration pathway from Amazon RDS into Aurora, with push-button migration tools that convert your existing RDS for PostgreSQL applications to Aurora PostgreSQL. Routine database tasks such as provisioning, patching, backup, recovery, failure detection, and repair are also easy to manage with Aurora PostgreSQL.\nAurora PostgreSQL can work with many industry standards. For example, you can use Aurora PostgreSQL databases to build HIPAA-compliant applications and to store healthcare related information, including protected health information (PHI), under a completed Business Associate Agreement (BAA) with AWS.\nAurora PostgreSQL is FedRAMP HIGH eligible. For details about AWS and compliance efforts, see AWS services in scope by compliance program.\n"
},
{
	"uri": "//localhost:1313/9-testfaulttolerance/9-1-setupfailovereventnotifications/",
	"title": "Set up failover event notifications",
	"tags": [],
	"description": "",
	"content": "To receive notifications when failover events occur with your DB cluster, you will create an Amazon Simple Notification Service (SNS) topic, subscribe your email address to the SNS topic, create an RDS event subscription publishing events to the SNS topic and registering the DB cluster as an event source.\nOpen a Cloud9 terminal window by referring Open Cloud9 Terminal Window section and paste the following command to create an SNS topic.\naws sns create-topic \\\r--name auroralab-cluster-failovers If successful, the command will respond back with a TopicArn identifier, you will need this value in the next command.\nNext, subscribe your email address to the SNS topic using the command below, changing the placeholder [YourEmail] with your email address:\naws sns subscribe \\\r--topic-arn $(aws sns list-topics --query \u0026#39;Topics[?contains(TopicArn,`auroralab-cluster-failovers`)].TopicArn\u0026#39; --output text) \\\r--protocol email \\\r--notification-endpoint \u0026#39;[YourEmail]\u0026#39; You should see Output similar to the following:\nYou will receive a verification email on that address, please confirm the subscription by following the instructions in the email.\nOnce you click Confirm subscription in the email, you\u0026rsquo;ll see a browser window with a confirmation message as follows:\nOnce confirmed, or while you are waiting for the verification email to arrive, create an RDS event subscription and register the DB cluster as an event source using the command below:\nIf your Aurora cluster name is different than aupg-labs-cluster, update the command below accordingly.\naws rds create-event-subscription \\\r--subscription-name auroralab-cluster-failovers \\\r--sns-topic-arn $(aws sns list-topics --query \u0026#39;Topics[?contains(TopicArn,`auroralab-cluster-failovers`)].TopicArn\u0026#39; --output text) \\\r--source-type db-cluster \\\r--event-categories \u0026#39;[\u0026#34;failover\u0026#34;]\u0026#39; \\\r--enabled aws rds add-source-identifier-to-subscription \\\r--subscription-name auroralab-cluster-failovers \\\r--source-identifier aupg-fcj-labs At this time the event notifications have been configured. Ensure you have verified your email address before proceeding to the next section.\n"
},
{
	"uri": "//localhost:1313/5-clustercachemanagement/5-1-setupclustercachemanagement/",
	"title": "Setup cluster cache management",
	"tags": [],
	"description": "",
	"content": "Configuring Cluster Cache Management (CCM) Following are the steps to configure and enable the use of CCM on your Aurora PostgreSQL cluster\nModify the Amazon Aurora DB Cluster Parameters related to CCM. Sign in to the AWS Management Console and select Parameter Groups on the Amazon RDS console.\nIn the list, choose the DB cluster parameter group for your Aurora PostgreSQL DB cluster. The DB cluster must use a parameter group other than the default, because you can\u0026rsquo;t change values in a default parameter group. For more information, see Creating a DB Cluster Parameter Group.\nClick Edit under the Actions menu.\nSet the value of the apg_ccm_enabled cluster parameter to 1 and click on Save Changes. For cluster cache management, make sure that the promotion priority is tier-0 for the writer DB instance of the Aurora PostgreSQL DB cluster. The promotion tier priority is a value that specifies the order in which an Aurora reader is promoted to the writer DB instance after a failure. Valid values are 0–15, where 0 is the first priority and 15 is the last priority. Select Databases in the Amazon RDS console.\nChoose the Writer DB instance of the Aurora PostgreSQL DB cluster and click on Modify The Modify DB Instance page appears. Under Additional configuration, choose tier-0 for Failover Priority. Choose Continue and check the summary of modifications.\nTo apply the changes immediately after you save them, choose Apply immediately and click Modify DB Instance to save your changes. For more information about setting the promotion tier, see Modify a DB Instance in a DB Cluster and the Promotion tier setting . See also Fault Tolerance for an Aurora DB Cluster.\nNext, set one reader DB instance for cluster cache management. To do so, choose a reader from the Aurora PostgreSQL cluster that is the same instance class and size as the writer DB instance. For example, if the writer uses db.r5.xlarge, choose a reader that uses this same instance class type and size. Then set its promotion tier priority to 0. The promotion tier priority is a value that specifies the order in which an Aurora replica is promoted to the primary DB instance after a failure. Valid values are 0 to 15, where 0 is the highest and 15 the lowest priority. In the navigation pane, choose Databases.\nChoose the Reader DB instance of the Aurora PostgreSQL DB cluster and click on Modify The Modify DB Instance page appears. Under Additional configuration, choose tier-0 for Failover Priority.\nChoose Continue and check the summary of modifications.\nTo apply the changes immediately after you save them, choose Apply immediately and click Modify DB Instance to save your changes. Verifying if CCM is enabled Click on the DB identifier with the cluster name you created as a part of the CloudFormation stack or manually.\nUnder Connectivity and Security section, you will notice 2 different endpoints. The one with type Writer is the cluster endpoint (for read-write connections) and the one with type Reader is the reader endpoint (for read-only connections). Open a cloud9 terminal window by referring Open Cloud9 Terminal Window section and using psql command line connect to the Aurora PostgreSQL DB cluster writer end point. Run the following SQL commands to check the cluster cache management status:\npsql \\x\rselect * from aurora_ccm_status(); If the Cluster Cache management is not enabled, querying aurora_ccm_status() will display the below output: aupglab=\u0026gt; \\x Expanded display is on. mylab=\u0026gt; select * from aurora_ccm_status(); ERROR: Cluster Cache Manager is disabled\n"
},
{
	"uri": "//localhost:1313/6-databaseactivitystreaming/6-1-setupkmsfordatabaseactivitystreamsinaction/",
	"title": "Setup KMS for Database Activity Streaming",
	"tags": [],
	"description": "",
	"content": "Database Activity Streaming requires a Master Key to encrypt the key that in turn encrypts the logged database activity. The Default AWS RDS KMS key can’t be used as the Master key. Therefore, we need to create a new customer managed KMS key to configure the Database Activity Streaming.\nCreate KMS Key Open KMS console and select Customer Managed Keys on the left-hand side and click on Create Key:\nOn the next screen under Configure key choose Symmetric key type and click Next:\nOn the next screen, under Add Labels give a name for the key under the field Alias such as cmk-apg-lab.\nUnder Description field, type a description for the key such as Customer managed Key for Aurora PostgreSQL Database Activity Streaming (DAS) lab and click Next.\nOn the next screen under Define Key Administrative permissions and Define key usage permissions, leave with default setting.\nOn the next screen, review the policy and click Finish.\nVerify the newly created KMS key on the KMS dashboard.\n"
},
{
	"uri": "//localhost:1313/5-clustercachemanagement/5-2-benchmarkingwithclustercachemanagement/",
	"title": "Benchmarking with Cluster Cache management",
	"tags": [],
	"description": "",
	"content": "Benchmarking with Cluster Cache management To verify the benefits of Cluster cache management feature on the Aurora cluster we will be performing the following steps. We will explore these steps in more detail in corresponding sections below.\nWith CCM enabled, we will run pgbench benchmark on the writer node. We\u0026rsquo;ll check cached pages on both writer and reader nodes to verify that they are kept in sync. Then we\u0026rsquo;ll Failover Aurora cluster. We\u0026rsquo;ll run pgbench benchmark on new writer node after the failover and verify that the TPS numbers between old and new writer nodes are similar. Then we\u0026rsquo;ll disable CCM. We\u0026rsquo;ll clear buffer cache of both writer and reader nodes by stopping and starting the cluster. With CCM disabled, we\u0026rsquo;ll run benchmark on writer node using pgbench again. We\u0026rsquo;ll check cached pages on both writer and reader nodes to verify that they are not kept in sync. Next, we\u0026rsquo;ll failover Aurora cluster one more time. We\u0026rsquo;ll run pgbench benchmark on the new writer node after failover and verify that the TPS numbers between old and new writer nodes vary significantly. Load large dataset for benchmarking In order to reproduce the benchmarking used in the lab, we need to add more sample data to our benchmark. The below command is using scale factor of 10000. This optional step could take ~15-20 mins to add more data to the database, so make sure you have enough time to complete the lab before running the script below.\nRun the following command in your Cloud9 terminal window to connect to the writer node of the Aurora cluster using pgbench and add sample data with scale factor = 10000\npgbench -i --fillfactor=100 --scale=10000 Check aupglab database size using PSQL.\npsql\rSELECT pg_size_pretty( pg_database_size(\u0026#39;aupglab\u0026#39;)); If you have used scale factor 100) in pgbench, you will see ourput similar to the following:\rIf you ran pgbench with the scale factor 10000 in the previous step, you will see output similar to the following:\rBenchmarking with CCM enabled Run benchmark on writer node using pgbench (before failover) Initiate a 600 seconds pgbench benchmarking on the Aurora PostgreSQL writer node with CCM enabled using the below command on your Cloud9 terminal window.\npgbench --progress-timestamp -M prepared -n -T 600 -P 5 -c 50 -j 50 -b tpcb-like@1 -b select-only@20 \u0026gt; ccm_enable_before_failover.out Explain command\rpgbench: This is a benchmarking tool for PostgreSQL databases.\n\u0026ndash;progress-timestamp: Prints a timestamp with each progress report.\n-M prepared: Uses prepared transactions for benchmarking.\n-n: Specifies a no-vacuum test, which avoids vacuuming during the benchmark.\n-T 600: Runs the benchmark for 600 seconds (10 minutes).\n-P 5: Populates the database with 5 client sessions before starting the benchmark.\n-c 50: Runs the benchmark with 50 concurrent client connections.\n-j 50: Specifies 50 threads for multi-threaded operation.\n-b tpcb-like@1: Uses the TPC-B-like transaction mix with a weight of 1.\n-b select-only@20: Uses a select-only transaction mix with a weight of 20.\nccm_enable_before_failover.out: Redirects the output to a file named \u0026ldquo;ccm_enable_before_failover.out\u0026rdquo;.\nWe are using pgbench benchmarking option tpcb-like and using “@” to specify the probability of running read-only workload and read-write workload. In the below example, we are running tpcb-like workload with 20X read-only workload and 1x read-write workload for 600 seconds.\nAfter 600 seconds when benchmark is complete, you can verify the pgbench output on the screen or refer the output file “ccm_enable_before_failover.out”. The summary output will look like below. Please note that your output may look a little different.\ncat ccm_enable_before_failover.out Check cached pages on both writer and reader nodes Pg_buffercache extension provides a means to look into the contents of the buffer cache. We will be leveraging the pg_buffercache view to examine the content of the buffer cache (with the CCM enabled and CCM disabled) to illustrate the effect of the CCM. We will compare the content of the buffer cache of the Writer node with the Reader node.\nWith CCM enabled, the content of the buffer cache of the writer and the reader node will be similar because the writer node will periodically send buffer addresses of the frequently used buffers (defaults to usage count\u0026gt;3) to the reader node to be read from storage.\nConnect to the Writer node with psql using the cluster endpoint of your cluster Create extension pg_buffercache on the Database.\npsql\rCREATE EXTENSION pg_buffercache;\r\\dx pg_buffercache Verify that you are connected to the Writer node and query pg_buffercache to see number of cached pages for various tables.\nshow transaction_read_only; SELECT c.relname, count(*) AS buffers\rFROM pg_buffercache b INNER JOIN pg_class c\rON b.relfilenode = pg_relation_filenode(c.oid) AND b.reldatabase IN (0, (SELECT oid FROM pg_database WHERE datname = current_database()))\rGROUP BY c.relname\rORDER BY 2 DESC\rLIMIT 10; Explain scripts\rCommand Purposes SELECT c.relname, count(*) AS buffers Queries for the relation name (relname) and the number of buffers used (count(*)), labeling the count as \u0026ldquo;buffers\u0026rdquo;. FROM pg_buffercache b Accesses data from the pg_buffercache system view, which holds details about buffered pages. INNER JOIN pg_class c Combines data from pg_buffercache with information about relations from the pg_class system catalog. ON b.relfilenode = pg_relation_filenode(c.oid) Links entries based on filenodes, associating buffers with their respective relations. AND b.reldatabase IN (0, (SELECT oid FROM pg_database WHERE datname = current_database())) Includes buffers for both shared system catalogs (database 0) and the current database. GROUP BY c.relname Aggregates results based on relation names. ORDER BY 2 DESC Sorts in descending order based on the number of buffers (count(*) in position 2) LIMIT 10; Restricts output to the top 10 relations. Connect to the Read replica Open the Amazon RDS console .\nIn the navigation pane, choose Databases and click on the name of the Aurora cluster you created.\nUnder Connectivity and Security tab, copy the Endpoint of type Reader.\nReplace below with the Aurora reader endpoint you copied above and using psql command line check if pg_buffercache extension is installed.\npsql -h \u0026lt;Aurora Reader EndPoint\u0026gt;\r\\dx pg_buffercache Verify if you are connected to the Reader node and query pg_buffercache to see number of cached pages for various tables.\nshow transaction_read_only; SELECT c.relname, count(*) AS buffers\rFROM pg_buffercache b INNER JOIN pg_class c\rON b.relfilenode = pg_relation_filenode(c.oid) AND\rb.reldatabase IN (0, (SELECT oid FROM pg_database\rWHERE datname = current_database()))\rGROUP BY c.relname\rORDER BY 2 DESC\rLIMIT 10; Notice that, after disabling CCM the buffer page count on the reader node is much less compared to the writer node for the frequently accessed tables.\nFailover Aurora cluster Now, we will initiate a failover of the Aurora cluster and after the failover is complete, we’ll run the same benchmark on the new writer node. For initiating failover go to the RDS console, select the writer instance of your Aurora cluster and click Failover in the Actions menu. Click Failover to confirm. Once the failover is complete (after about ~30 seconds), verify that the previous reader node becomes the new writer. Run benchmark using pgbench on new writer node (after failover) Now we’ll run the same pgbench benchmark as we did earlier and we will compare the metrics observed before and after the failover. pgbench --progress-timestamp -M prepared -n -T 600 -P 5 -c 50 -j 50 -b tpcb-like@1 -b select-only@20 \u0026gt; ccm_enable_after_failover.out After 600 seconds when benchmark is complete, you can verify the pgbench output on the screen or refer the output file “ccm_enable_after_failover.out”. The summary output will look like below. Please note that your output may look a little different. cat ccm_enable_after_failover.out Notice that after disabling CCM, the tps numbers on the new writer node after failover is significantly less compared to the old writer node before the failover.\nBenchmarking with CCM disabled Now, we will disable CCM and perform similar tests as we did above with CCM enabled.\nDisable CCM Disable CCM on the Aurora PostgreSQL cluster by modifying the cluster parameter group and setting the value of apg_ccm_enabled parameter to 0.\nOpen the Amazon RDS console and select Parameters groups .\nIn the list, choose the parameter group for your Aurora PostgreSQL DB cluster. ] Click on the DB cluster parameter group selected above and then click on Edit Parameters. Set the value of the apg_ccm_enabled cluster parameter to 0 and click on Save changes. Verify Cluster Cache Management is disabled by querying the function aurora_ccm_status() as shown below:\npsql\r\\x\rselect * from aurora_ccm_status(); Clear buffer cache of both writer and reader nodes Since earlier testing with CCM already warmed the buffer cache of the reader and writer instances, we need to stop and start the Aurora cluster to empty the buffer caches before running the next benchmarking. We could also reboot both the reader and the writer instances, but this may not guarantee that the writer and reader will come up with empty buffer cache.\nTo stop the cluster Verify that the cluster status is shown as “Available”, then click on the Actions menu and choose Stop temporarily. Confirm the action by clicking Stop temporarily database. It will take several minutes and the cluster status will change from Stopping to Stopped. To start the cluster Once the cluster status changes to ”Stopped”, click on the Actions menu again and choose Start. It will take several minutes and the cluster status will change from \u0026ldquo;Starting\u0026rdquo; to \u0026ldquo;Available\u0026rdquo;.\nRun benchmark on writer node using pgbench (before failover) Run benchmark using pgbench on the Aurora cluster writer node like earlier. pgbench --progress-timestamp -M prepared -n -T 600 -P 5 -c 50 -j 50 -b tpcb-like@1 -b select-only@20 \u0026gt; ccm_disable_before_failover.out After 600 seconds when benchmark is complete, you can verify the pgbench output on the screen or refer the output file “ccm_disable_before_failover.out”. The summary output will look like below. Please note that your output may look a little different. cat ccm_disable_before_failover.out Check cached pages on both writer and reader nodes Connect to the Writer node using the cluster endpoint of your cluster psql\r\\dx pg_buffercache Verify that you are connected to the Writer node and query pg_buffercache to see number of cached pages for various tables. show transaction_read_only; SELECT c.relname, count(*) AS buffers\rFROM pg_buffercache b INNER JOIN pg_class c\rON b.relfilenode = pg_relation_filenode(c.oid) AND\rb.reldatabase IN (0, (SELECT oid FROM pg_database\rWHERE datname = current_database()))\rGROUP BY c.relname\rORDER BY 2 DESC\rLIMIT 10; Connect to the read replica Connect to the reader instance using the Reader Endpoint of the Aurora PostgreSQL Cluster. Replace below with your Aurora reader endpoint by referring the step Connect to the Read replica. psql -h \u0026lt;Aurora Reader EndPoint\u0026gt;\r\\dx pg_buffercache Verify that you are connected to the Reader node and query pg_buffercache to see number of cached pages for various tables. show transaction_read_only; SELECT c.relname, count(*) AS buffers\rFROM pg_buffercache b INNER JOIN pg_class c\rON b.relfilenode = pg_relation_filenode(c.oid) AND\rb.reldatabase IN (0, (SELECT oid FROM pg_database\rWHERE datname = current_database()))\rGROUP BY c.relname\rORDER BY 2 DESC\rLIMIT 10; Notice that, after disabling CCM the buffer page count on the reader node is much less compared to the writer node for the frequently accessed tables.\nFailover Aurora cluster Now, we will initiate a failover of the Aurora cluster and after the failover is complete, we’ll run the same benchmark on the new writer node. Confirm the action by clicking Failover. Once the failover is complete (after about ~30 seconds), verify that the previous reader node becomes the new writer.\nRun benchmark using pgbench on new writer node (after failover) Now, we’ll run the same benchmarking as we did earlier and we will compare the pgbench metrics observed before and after the failover. pgbench --progress-timestamp -M prepared -n -T 600 -P 5 -c 50 -j 50 -b tpcb-like@1 -b select-only@20 \u0026gt; ccm_disable_after_failover.out After 600 seconds when benchmark is complete, you can verify the pgbench output on the screen or refer the output file “ccm_disable_after_failover.out”. The summary output will look like below. Please note that your output may look a little different.\ncat ccm_disable_after_failover.out Notice that after disabling CCM, the tps numbers on the new writer node after failover is significantly less compared to the old writer node before the failover.\n"
},
{
	"uri": "//localhost:1313/2-preparation/2-2-createsg/",
	"title": "Create Security Group ",
	"tags": [],
	"description": "",
	"content": "To create an EC2 security group in the AWS console: Go to the EC2 console.\nIn the navigation pane, choose Security Groups.\nChoose Create Security Group.\nFor VPC, choose the VPC where you want to create the security group.\nFor Security group name, enter a descriptive name for the security group.\nFor Description, enter a description for the security group. Modify Inbound Rule \u0026amp; Outbound Rule Click Create. Once you have created the security group for EC2 instances\nTo create an Database security group in the AWS console: Go to the EC2 console.\nIn the navigation pane, choose Security Groups.\nChoose Create Security Group.\nFor VPC, choose the VPC where you want to create the security group.\nFor Security group name, enter a descriptive name for the security group.\nFor Description, enter a description for the security group. Modify Inbound Rule \u0026amp; Outbound Rule Click Create. Once you have created the security group for Aurora PostgreSQL\n"
},
{
	"uri": "//localhost:1313/6-databaseactivitystreaming/6-2-dbactivitystreamsinaction/",
	"title": "Database Activity Streams in action",
	"tags": [],
	"description": "",
	"content": "Database Activity Streams provide a near real-time data stream of the database activity in your relational database. When you integrate Database Activity Streams with third-party monitoring tools, you can monitor and audit database activity.\nIn this section of the lab, first we\u0026rsquo;ll configure and start database activity streams, and then we\u0026rsquo;ll generate some load and observe the database activity streams output.\nConfiguring Database Activity Streams You start an activity stream at the DB cluster level to monitor database activity for all DB instances of the cluster. Any DB instances added to the cluster are also automatically monitored.\nYou can choose to have the database session handle database activity events either synchronously or asynchronously:\nSynchronous mode: In synchronous mode, when a database session generates an activity stream event, the session blocks until the event is made durable. If the event can\u0026rsquo;t be made durable for some reason, the database session returns to normal activities. However, an RDS event is sent indicating that activity stream records might be lost for some time. A second RDS event is sent after the system is back to a healthy state. The synchronous mode favors the accuracy of the activity stream over database performance.\n2.** Asynchronous mode**: In asynchronous mode, when a database session generates an activity stream event, the session returns to normal activities immediately. In the background, the activity stream event is made a durable record. If an error occurs in the background task, an RDS event is sent. This event indicates the beginning and end of any time windows where activity stream event records might have been lost.\nAsynchronous mode favors database performance over the accuracy of the activity stream.\nStart activity streams Open the Amazon RDS service console Databases section .\nSelect the Aurora DB cluster that was you created manually.\nClick Actions menu and choose Start activity stream.\nEnter the following settings in the Database Activity Stream window:\nFor AWS KMS key, choose the key that you created in the earlier step. If you don\u0026rsquo;t see the new key - try to refresh the browser window.\nFor Database activity stream mode, choose Asynchronous.\nChoose Apply immediately.\nThe Status column on the RDS, Database page for the cluster will start showing configuring-activity-stream.\nVerify the activity streaming by clicking on the cluster name and clicking on configuration. You will see the Kinesis stream name to which the Database Activity Stream will be published.\nWait till the status on RDS, Database page for the cluster changes back to Available. It might take upto 10 minutes for the status to change.\nGenerate load on the Aurora cluster Open a Cloud9 terminal window by referring Open Cloud9 Terminal Window section and run pgbench.\npgbench --protocol=prepared --progress=60 --time=300 --client=16 --jobs=96 \u0026gt; results1.log Sample code to view Database Activity Streams Open a Cloud9 terminal window, download a sample python script das-script.py, following command below:\nwget https://aupg-fcj-assets.s3.us-west-2.amazonaws.com/lab-scripts/das-scripts.py In this script, you will be required to replace the value for REGION_NAME as per the AWS Region you are running this lab for e.g. us-west-2 and RESOURCE_ID with the Aurora cluster\u0026rsquo;s Resource id value\nOpen a new Cloud9 terminal window and paste the following to edit the Python script:\nnano /home/ec2-user/das-script.py Update the following variables (REGION_NAME and RESOURCE_ID ) in the script as per your actual settings\nTo save file after changes in nano editor, press CTRL-X , enter Y and then Enter.\nTo view the database activity stream, run the python script as shown below:\npython3 /home/ec2-user/das-script.py You will see a lot of messages in the terminal output which is in JSON format.\nSample Output Activity Streaming To format the Database Activity Streaming output and interpret the results, you can use a free tool like JSON formatter .\nCopy a block of the das-script.py script output starting from {\u0026ldquo;type\u0026rdquo;: and ending with } as shown in the below screenshot and paste it into JSON formatter . Then press the Format / Beautify button. You should see the formatted database activity similar to the following:\nStopping a Database Activity Streaming Open the Amazon RDS console .\nIn the navigation pane, choose Databases and select the Aurora DB cluster that you created manually.\nClick on Action and select Stop database activity stream.\nChoose Apply immediately and click Continue to stop Database activity streaming on the cluster.\nThe status column on the RDS Database home page for the cluster will start showing configuring-activity-stream.\nAfter some time, activity streams will be stopped and the status column on the RDS Database home page for the cluster will change back to Available.\n"
},
{
	"uri": "//localhost:1313/2-preparation/",
	"title": "Preparation Steps",
	"tags": [],
	"description": "",
	"content": "The preparation steps using several components:\nAmazon VPC network configuration with public and private subnets Database subnet group and relevant security groups for the Aurora PostgreSQL cluster and AWS Cloud9 environment AWS Cloud9 configured with the software components needed for the labs Custom cluster and DB instance parameter groups for the Amazon Aurora PostgreSQL cluster, enabling some extensions and useful parameters The master database credentials will be store in AWS Secrets Manager Content Create VPC Create Security Group Create EC2 Create Subnet Group and Parameter Group Create Aurora PostgreSQL Cluster Configure Cloud9 and Initialize Database "
},
{
	"uri": "//localhost:1313/9-testfaulttolerance/9-2-testamanualdbclusterfailover/",
	"title": "Test a manual DB cluster failover",
	"tags": [],
	"description": "",
	"content": "In this test, we will use a Python script to connect to the cluster endpoint and continuously run a monitoring query.\nYou will need to open an additional Cloud9 terminal window as shown below. You will execute commands in one and see the results in the other session.\nDownload the failover test script with command below:\nwget https://aupg-fcj-assets.s3.us-west-2.amazonaws.com/lab-scripts/simple_failover.py In one of the two terminal windows, run the failover test script using the following command:\npython /home/ec2-user/simple_failover.py -e $DBENDP -u $DBUSER -p $DBPASS -d $PGDATABASE In the second cloud9 window, execute the command to initiate the failover\naws rds failover-db-cluster --db-cluster-identifier aupg-fcj-labs Initially the script would be connecting to the writer node and executing the query. You will see a slight pause and a message \u0026ldquo;waiting for failover\u0026rdquo; when the failover is initiated. Subsequently the time elapsed to re-connect and the new writer node information is printed.\nSince we are using the cluster endpoint for the connection, there is a slight delay to propagate the DNS record changes for the new writer node. You will see that for a few seconds after the failover, we are still connected to the old writer node which is now the new reader node. Once the DNS record change for the cluster endpoint is propagated to the client machine (in this case the Cloud9 workstation), the script will indicate that it is connected to the Writer node.\nYou will receive two event notification emails for each failover you initiate, one indicating that a failover has started, and one indicating that it has completed.\n"
},
{
	"uri": "//localhost:1313/2-preparation/2-3-createec2/",
	"title": "Create a EC2 instance",
	"tags": [],
	"description": "",
	"content": "To create an EC2 instance for an Cloud9 enviroment, follow these steps: Open the Amazon EC2 console.\nClick Launch Instance. Enter name your EC2 instance. Choose an AMI. For an app server, you can choose a Linux AMI, such as Amazon Linux 2.\nChoose an instance type. The instance type that you choose will depend on the requirements of your app server. For example, if you are running a high-traffic website, you will need a larger instance type with more CPU and memory. For Key Pair,choose your keypair you have created or click Create new key pair Configure the instance details. This includes things like the number of instances to launch, the network configuration, and the storage configuration.\nFor Network settings\nChoose VPC which contains EC2 app server Choose Subnet Enable Auto-assign public IP Add a security group for EC2 app server that you have created easier step . A security group is a firewall that controls incoming and outgoing traffic to your instance. Review and launch the instance Once the instance is launched, you can connect to it using an SSH client, such as MobaXterm or Putty. Once you are connected, you can install your app server and deploy your application.\n"
},
{
	"uri": "//localhost:1313/3-fastcloning/",
	"title": "Fast Cloning",
	"tags": [],
	"description": "",
	"content": "What\u0026rsquo;s the Fast Cloning ? Fast Cloning\rFast cloning refers to a feature that allows you to quickly and efficiently create an exact copy of your database. This clone shares the same underlying storage with the original database initially, making the process much faster and more cost-effective than traditional methods like full database backups or restoring snapshots.\nHere are some key points about fast cloning for Aurora PostgreSQL:\nSpeed: Unlike traditional methods, which can take hours or even days for large databases, fast cloning can create a clone in a matter of minutes, even for multi-terabyte databases.\nEfficiency: The clone initially shares the storage with the source database, meaning no physical data needs to be copied. This saves disk space and reduces storage costs.\nCopy-on-write: When changes are made to either the source database or the clone, new data pages are created instead of modifying existing ones. This ensures data consistency and minimizes impact on both databases.\nMultiple uses: Fast cloning is useful for various scenarios, including application development and testing, database updates, blue/green deployments, and running analytical queries without impacting production. Here are some resources where you can learn more about fast cloning for Aurora PostgreSQL:\nAmazon Aurora documentation Amazon Aurora PostgreSQL blue/green deployment using fast database cloning Amazon Aurora Fast Database Cloning\nIn this lab, we will walk through the process of creating an Aurora fast clone. We will observe the divergence of data and compare the performance between the original and cloned Aurora clusters.\nSetting up the Fast Clone divergence test Lets review the tables we have created in the Create, Configure Cloud9 and Initialize Database step, which will be used in this lab.\nResoures name Value cloneeventtest Table to store the counter and the timestamp statusflag Table to store the status flag which controls the start/stop counter eventerrormsg Table to store error messages cloneeventproc Function to add data to the cloneeventtest table based on the start counter flag Creating and verifying performance impact of the Fast Clone Running the pgbench workload on the primary cluster\nBefore creating a Fast Clone of the primary cluster, we are going to start pgbench test to measure the Transaction per seconds (TPS) metrics on the primary cluster. Open a Cloud9 terminal window (terminal #1) by referring Open Cloud9 Terminal Window section and run the following command. This will run for 30 minutes.\npgbench --progress-timestamp -M prepared -n -T 1800 -P 60 -c 8 -j 8 -b tpcb-like@1 -b select-only@20 \u0026gt; Primary_results.log Verify the environment and run the sample divergence test\nIn order to verify the data divergence on the primary and the clone cluster, we will be adding sample data using sample tables.\nWe need to open one more Cloud9 terminal window (terminal #2) to connect to Aurora and run the function. To open one more terminal window on your Cloud9 environment, click on Window menu and select new Terminal.\nRun the following commands to verify the delflag column is set to 0 in the statusflag table and there is no data in the table cloneeventtest. Execute the function cloneeventproc() to start adding sample data.\npsql\rselect * from statusflag;\rselect * from cloneeventtest;\rselect cloneeventproc(); At this time (we call as time “T1”), the pgbench workload is running on the source DB cluster and also, we are adding sample data to the table on the primary cluster every 10 seconds.\nStop the sample data generation First, at T1+5 minutes we will stop the function execution by manually resetting the delflag column on the table statusflag to 1. Open one more Cloud9 terminal window to connect to Aurora (terminal #3). The pgbench workload will continue to execute on the primary source cluster in terminal #1.\npsql\rupdate statusflag set delflag=\u0026#39;Y\u0026#39;; Go back to terminal #2 where we ran cloneeventproc function. Wait for ~60sec until you see the function complete its execution:\nselect cloneeventproc(); Let’s check the number of rows in the table cloneeventtest. We should see 5 or more rows in the table:\nselect count(*) from cloneeventtest; Let\u0026rsquo;s set proper timezone and check rows in cloneeventtest table:\nSET timezone = \u0026#39;Asia/Ho_Chi_Minh\u0026#39;;\rselect * from cloneeventtest; Create Fast Clone Cluster Once the function execution is stopped (after time T1+5 minutes) we will start creating the Fast Clone of the primary cluster. The pgbench workload on the primary will continue on the primary cluster in terminal#1.\nNow, we will walk you through the process of cloning a DB cluster. Cloning creates a separate, independent DB cluster, with a consistent copy of your data set as of the time you cloned it. Database cloning uses a copy-on-write protocol, in which data is copied at the time that data changes, either on the source databases or the clone databases. The two clusters are isolated, and there is no performance impact on the source DB cluster from database operations on the clone, or the other way around.\nFollowing are the steps to configure the Database Fast clone on your Aurora PostgreSQL cluster:\na. Sign in to the AWS Management Console and open the Amazon RDS console .\nb. In the navigation pane, choose Databases and select DB identifier with the cluster name you created as a part of the CloudFormation stack. Click the Actions menu at the top and select Create clone.\n![FC](/images/3/9.png)\rc. Enter aupglabs-clone as DB Instance Identifier and Capacity type Provisioned.\nFor Cluster storage configuration select Aurora Standard.\nIn Instance configuration select Memory optimized classes and db.r6g.large. In the Connectivity section, leave with default setting\nIn Additional Configuration, pick the DB cluster parameter group and DB parameter groups created in DB cluster parameter group and DB parameter group drop down menus. Enable auto minor version upgrade.\nLeave rest of the input fields at their default value and click Create clone. Once you click on the “Create Clone” the status column will show status as “Creating”. The clone cluster should be ready after about 10-15 minutes or so. The status column will show as “Available” once the cloned cluster is ready.\nStart the sample data divergence process on the primary cluster Once the Clone cluster creation process is kicked off, we will start the sample data generation process on the primary cluster. Any sample data added from this point onwards should only be available on the primary cluster and not on the clone cluster.\npsql truncate cloneeventtest; # This will empty the cloneeventtest table, removing all existing rows.Make sure you want to do this as it\u0026#39;s an irreversible operation.\rupdate statusflag set delflag=0; select count(*) from cloneeventtest;\rselect cloneeventproc(); Verify the data divergence on the Clone Cluster Clone cluster should be ready after about 15 minutes or so (time T1+~10-15 minutes).\nThe table “cloneeventtest” on the cloned cluster should have the snapshot of data as it existed on the primary cluster at ~T1+5, because that is when we started creating the clone.\nCopy the Writer Endpoint for your cloned aurora cluster by clicking the cluster name and going to the Connectivity \u0026amp; security tab.\nConnect to Aurora cloned cluster from session#3 window. Replace below with the Writer endpoint for your Cloned Aurora cluster you copied above.\npsql -h \u0026lt;Cloned Cluster Writer Endpoint\u0026gt; And run the sql commands to check the content of the data:\nselect count(*) from cloneeventtest;\rSET timezone = \u0026#39;Asia/Ho_Chi_Minh\u0026#39;;\rselect * from cloneeventtest; Stop the function run on Primary aurora cluster that is currently running (follow step Stop the sample data generation ) and select data from cloneeventtest table. We will see more rows, as expected.\nRun the pgbench workload on the Clone Cluster We are going to start a similar pgbench workload on the newly created clone cluster as we did on the primary cluster earlier in step#1. Replace below with the Writer endpoint for your Cloned Aurora cluster.\npgbench --progress-timestamp -M prepared -n -T 1800 -P 60 -c 8 -j 8 --host=\u0026lt;Cloned Cluster Writer Endpoint\u0026gt; -b tpcb-like@1 -b select-only@20 \u0026gt; Clone_results.log Verify the pgbench metrics on the primary and the clone cluster. Once the pgbench workload completes on both the primary and the clone cluster, we can verify the TPS metrics from both the clusters by looking at the output file. "
},
{
	"uri": "//localhost:1313/9-testfaulttolerance/9-3-testingfaultinjectionqueries/",
	"title": "Testing fault injection queries",
	"tags": [],
	"description": "",
	"content": "In this test you will simulate a crash of the database engine service on the DB instance. This type of crash can be encountered in real circumstances as a result of out-of-memory conditions, or other unexpected circumstances.\nLearn more about fault injection queries\rFault injection queries provide a mechanism to simulate a variety of faults in the operation of the DB cluster. They are used to test the tolerance of client applications to such faults. They can be used to:\nSimulate crashes of the critical services running on the DB instance. These do not typically result in a failover to a reader, but will result in a restart of the relevant services. Simulate disk subsystem degradation or congestion, whether transient in nature or more persistent. Simulate read replica failures In one of the two terminal windows, run the failover test script using the following command:\npython /home/ec2-user/simple_failover.py -e $DBENDP -u $DBUSER -p $DBPASS -d $PGDATABASE Since we are using the cluster endpoint to connect, the motioning script is connected to the current writer node.\nOn the other Cloud9 terminal window, issue the following fault injection command. A crash of the PostgreSQL-compatible database for the Amazon Aurora instance will be simulated.\npsql -c \u0026#34;SELECT aurora_inject_crash (\u0026#39;instance\u0026#39;);\u0026#34; Wait and observe the monitoring script output. Once the crash is triggered, you should see an output similar to the example below.\nAs you see above, the instance was restarted and the monitoring script reconnected after a brief interruption.\n"
},
{
	"uri": "//localhost:1313/2-preparation/2-4-createpgsg/",
	"title": "Create Subnet Group and Parameter Group",
	"tags": [],
	"description": "",
	"content": "To create a DB subnet group on AWS, follow these steps: Open the Amazon RDS console\nIn the navigation pane, choose Subnet groups and click on Create DB Subnet Group. For Name and Description, type in a name and description for your DB subnet group.\nFor VPC, select the VPC in which you want to create your DB subnet group. Select the subnets that you want to include in your DB subnet group. Make sure to select subnets in at least two different Availability Zones (AZs). Click Create.\nYour DB subnet group will be created and will be displayed in the list of DB subnet groups.\nHere are some additional things to keep in mind when creating a DB subnet group:\nYou can only create a DB subnet group in a VPC that is in the same AWS Region as the database engine that you plan to use. You must include at least one subnet in each AZ in which you want to deploy your DB instance. You cannot modify a DB subnet group once it has been created. If you need to change the subnets in your DB subnet group, you must create a new one. You can use a DB subnet group to create a DB instance in any AZ in the VPC. To create a Database Parameter Group Go to the AWS RDS console, select Parameter Group, choose Create parameter group. In the Parameter group details section\nFor Parameter group family, select aurora-postgresql15 For Type, select DB Parameter Group For Group Name, type your parameter group name For Description, type your description Click Create Change and enable some configurations in the parameter group Go to Parameter Group console, click Action, choose Edit Fill shared_preload_libraries parameter in the search bar, then select shared_preload_libraries Click Save Changes\nAbout shared_preload_libraries parameter The shared_preload_libraries parameter plays a crucial role in configuring your Aurora PostgreSQL instance. Purpose:\nThis parameter specifies which shared libraries are preloaded when your Aurora PostgreSQL server starts. Preloading libraries reduces the overhead of loading them on-demand when needed, potentially improving performance for specific functionalities.\nWhat libraries are preloaded?\nBuilt-in libraries: Certain core PostgreSQL functionalities rely on shared libraries that are automatically preloaded by default. You don\u0026rsquo;t need to configure them in shared_preload_libraries. Custom libraries: You can specify additional shared libraries to be preloaded for specific needs. These could be: PostgreSQL extensions: For features like full-text search or geospatial data handling. Custom modules: Developed by you or third-party vendors for unique functionalities. Important Notes:\nPerformance impact: Preloading unnecessary libraries can consume memory and negatively affect startup times. Only preload libraries actively used by your applications. Security considerations: Be cautious when adding custom libraries due to potential security vulnerabilities. Ensure they are from trusted sources and vetted carefully. Restart requirement: Modifying shared_preload_libraries requires restarting your Aurora PostgreSQL instance for the changes to take effect.\nTo create a Database Cluster Parameter Group Go to the AWS RDS console, select Parameter Group, choose Create parameter group. In the Parameter group details section\nFor Parameter group family, select aurora-postgresql15 For Type, select DB Cluster Parameter Group For Group Name, type your parameter group name For Description, type your description Click Create Change and enable some configurations in the parameter group Go to Parameter Group console, click Action, choose Edit Fill log_rotation parameter in the search bar,\nEdit log_rotation_age value: 1440\nEdit log_rotation_size value: 102400\nClick Save Changes\nAbout log_rotation_age \u0026amp; log_rotation_size parameter\nlog_rotation_age\rFunction:\nlog_rotation_age defines the maximum age in minutes for individual log files within the cluster. Once a file reaches this age, it\u0026rsquo;s automatically rotated and a new one is created. This parameter helps manage disk space by preventing log files from growing indefinitely.\nConfiguration:\nUnlike a standalone instance where you can set log_rotation_age in the postgresql.conf file or via command line, this parameter needs to be configured through a custom Aurora PostgreSQL parameter group. You can set the desired value within the parameter group and apply it to your cluster.\nImpact:\nSetting a shorter log_rotation_age value results in more frequent rotations and fresher log files, but can also increase disk I/O activity and potentially impact performance. A longer age reduces file rotations but retains older logs, which might not be necessary for your needs.\nImportant Notes:\nRDS log rotation feature: While the RDS console lists log_rotation_age as a configurable parameter for its built-in log rotation feature, it currently has no effect on Aurora PostgreSQL clusters. You still need to use a custom parameter group to control file age rotation. CloudWatch Logs integration: Aurora PostgreSQL automatically streams logs to CloudWatch Logs by default. You can configure age-based retention policies within CloudWatch to manage the overall lifespan of log data, regardless of file rotations.\nRecommendations:\nChoose a log_rotation_age value that balances the need for disk space management with keeping sufficient log history for troubleshooting and analysis. Consider monitoring your cluster performance and adjusting the parameter value if necessary. Utilize CloudWatch Logs for long-term log retention and analysis beyond the file rotations managed by log_rotation_age. log_rotation_size\rFunction:\nlog_rotation_size specifies the maximum size (in kilobytes) for an individual log file before it\u0026rsquo;s automatically rotated and a new one is created. This helps prevent excessive log growth and keeps the log directory manageable.\nConfiguration:\nSimilar to log_rotation_age, you need to configure this parameter through a custom Aurora PostgreSQL parameter group. Set the desired size in kilobytes within the parameter group and apply it to your cluster.\nImpact:\nChoosing a smaller log_rotation_size value will trigger more frequent rotations, meaning smaller and fresher log files. However, this can increase disk I/O activity and impact performance slightly. Conversely, a larger size leads to fewer rotations and potentially larger logs, but may consume more disk space.\nImportant Notes:\nAutomatic file naming: As files are rotated, Aurora PostgreSQL adds timestamps or sequence numbers to their filenames to maintain historical context. RDS log rotation feature: Similar to log_rotation_age, the RDS log rotation feature doesn\u0026rsquo;t currently control size-based rotation in Aurora PostgreSQL clusters. The custom parameter group approach is necessary. CloudWatch Logs integration: You can use CloudWatch Logs with its size-based retention policies to archive or delete rotated log files after exceeding a specific size. Recommendations:\nSelect a log_rotation_size value that balances disk space management with your log analysis needs. Consider the volume and size of your typical logs to estimate appropriate rotation intervals. Monitor your cluster performance and adjust the parameter value if necessary to avoid excessive rotations or large log files. Leverage CloudWatch Logs for long-term log retention and management, independent of rotations controlled by log_rotation_size. Remember, optimizing both log_rotation_age and log_rotation_size allows you to effectively manage your Aurora PostgreSQL cluster\u0026rsquo;s log files, ensuring sufficient data for analysis while limiting disk usage and potential performance impacts. "
},
{
	"uri": "//localhost:1313/4-queryplanmanagement/",
	"title": "Query Plan Management",
	"tags": [],
	"description": "",
	"content": "With query plan management (QPM), you can control execution plans for a set of statements that you want to manage. You can do the following:\nImprove plan stability by forcing the optimizer to choose from a small number of known, good plans. Optimize plans centrally and then distribute the best plans globally. Identify indexes that aren\u0026rsquo;t used and assess the impact of creating or dropping an index. Automatically detect a new minimum-cost plan discovered by the optimizer. Try new optimizer features with less risk, because you can choose to approve only the plan changes that improve performance. For additional details on the Query Plan Management please refer official documentation Managing Query Execution Plans for Aurora PostgreSQL .\nThe quickest way to enable QPM is to use the automatic plan capture, which enables the plan capture for all SQL statements that run at least two times. In this lab, we will walk through the process of enabling QPM with automatic plan capture, evolving captured query plans to manually accept them and fixing query plans by using optimizer hints.\nQuick start guide on using QPM with automatic capture Here are the steps to configure and enable QPM on your Aurora PostgreSQL cluster to automatically capture and control execution plans for a set of SQL statements.\nModify the Amazon Aurora DB Cluster Parameters related to the QPM. Open the Amazon RDS service console Parameters group section located on left-hand panel of RDS console.\nIn the list, choose the parameter group for your Aurora PostgreSQL DB cluster. The DB cluster must use a parameter group other than the default, because you can\u0026rsquo;t change values in a default parameter group. For more information, see Creating a DB Cluster Parameter Group.\nClick Edit under the Actions menu.\n![QMP](/images/4.1/1.png)\rIn Parameter Filter field, enter rds.enable_plan_management without any spaces to reveal the filtered parameter. Set value of rds.enable_plan_management to 1 and click on Save changes.\n![QMP](/images/4.1/2.png)\rClick on the database parameter group name DB instance parameter group and click Edit.\n![QMP](/images/4.1/3.png)\rWe need to change two paramaters:\nModify the value for apg_plan_mgmt.capture_plan_baselines parameter to automatic\nModify the value for apg_plan_mgmt.use_plan_baselines to true\nClick Save Changes to save changes\nClick Databases on the left navigation panel and wait for the status of the instance to change to available. The parameter changes will take effect after reboot as suggested on the configuration tab of the Aurora writer and reader instances.\nReboot the writer and reader nodes by selecting it and going to the Actions menu.\nWait for the Status of Writer and Reader nodes to become Available.\nCreate and verify the apg_plan_mgmt extension for your DB instance. Open a Cloud9 terminal window by referring Open Cloud9 Terminal Window section and create the apg_plan_mgmt extension for your DB instance.\n```\rpsql\rCREATE EXTENSION apg_plan_mgmt;\rselect extname,extversion from pg_extension where extname='apg_plan_mgmt';\r```\rYou should see output similar to the following. The extension version will vary depending on the Aurora PostgreSQL version.\n![QMP](/images/4.1/5.png)\rReview all QPM related parameters are modified to the appropriate value by pasting the following queries.\n```\rshow apg_plan_mgmt.capture_plan_baselines;\rshow apg_plan_mgmt.use_plan_baselines;\r\\q\r```\r![QMP](/images/4.1/6.png)\rRun synthetic workload with automatic capture. Open a terminal window in Cloud9 and run pgbench (a PostgreSQL benchmarking tool) to generate a simulated workload, which runs same queries for a specified period. With automatic capture enabled, QPM captures plans for each query that runs at least twice.\n```\rpgbench --progress-timestamp -M prepared -n -T 100 -P 1 -c 100 -j 100 -b tpcb-like@1 -b select-only@20\r# The following pgbench command runs for 100 seconds with 100 clients/db sessions and 100 job threads emitting progress every 1 second\r```\rWait for the above command to finish.\nOpen another terminal window on Cloud9 to query apg_plan_mgmt.dba_plans table to view the managed statements and the execution plans for the SQL statements started with the pgbench tool. Then run the following commands: psql\rSELECT sql_hash, plan_hash, status, enabled, sql_text FROM apg_plan_mgmt.dba_plans; Turn off automatic capture of query plans. Capturing all plans with automatic capture has little runtime overhead and can be enabled in production. We are turning off the automatic capture to make sure that we don’t capture SQL statements outside the pgbench workload. This can be turned off by setting the apg_plan_mgmt.capture_plan_baselines parameter to off from the DB parameter group. Verify parameter settings using PSQL. As apg_plan_mgmt.capture_plan_baselines is a dynamic parameter, it doesn\u0026rsquo;t need an instance reboot to take effect. It will take a 5-10 seconds for the parameter value to change. show apg_plan_mgmt.capture_plan_baselines; Let’s verify that the execution plan for one of the managed statements is same as the plan captured by QPM. Execute explain plan on one of the managed statements. The explain plan output shows that the SQL hash and the plan hash matches with the QPM approved plan for that statement. explain (hashes true) UPDATE pgbench_tellers SET tbalance = tbalance + 100 WHERE tid = 200; # (hashes true): This is likely part of a specific testing or benchmarking framework and doesn\u0026#39;t directly affect the UPDATE statement itself. It might indicate a particular flag or configuration within the framework. In addition to automatic plan capture, QPM has manual plan capture capability, which offers a mechanism to capture execution plans for known problematic queries. Capturing the plans automatically is recommended generally. However, there are situations where capturing plans manually would be the best option, such as:\nYou don\u0026rsquo;t want to enable plan management at the Database level, but you do want to control a few critical SQL statements only. You want to save the plan for a specific set of literals or parameter values that are causing a performance proble QPM Plan adaptability with plan evolution mechanism If the optimizer\u0026rsquo;s generated plan is not a stored plan, the optimizer captures and stores it as a new unapproved plan to preserve stability for the QPM-managed SQL statements. Query plan management provides techniques and functions to add, maintain, and improve execution plans and thus provides Plan adaptability. Users can instruct QPM on demand or periodically to evolve all the stored plans to see if there is a better minimum cost plan available than any of the approved plans.\nQPM provides apg_plan_mgmt.evolve_plan_baselines function to compare plans based on their actual performance. Depending on the outcome of your performance experiments, you can change a plan\u0026rsquo;s status from unapproved to either approved or rejected. You can instead decide to use the apg_plan_mgmt.evolve_plan_baselines function to temporarily disable a plan if it does not meet your requirements. For additional details about the QPM Plan evolution, see Evaluating Plan Performance .\nFor the first use case, we’ll walk through an example on how QPM helps ensure plan stability where various changes can result in plan regression.\nIn most cases, you set up QPM to use automatic plan capture so that plans are captured for all statements that run two or more times. However, you can also capture plans for a specific set of statements that you specify manually. To do this, you set apg_plan_mgmt.capture_plan_baselines = off in the DB parameter group (which is the default) and apg_plan_mgmt.capture_plan_baselines = manual at the session level.\nEnable manual plan capture to instruct QPM to capture the execution plan of the desired SQL statements manually. SET apg_plan_mgmt.capture_plan_baselines = manual; Run explain plan for a specific query so that QPM can capture the execution plan (the following output for the explain plan is truncated for brevity). explain (analyze, summary, hashes)\rSELECT Sum(delta),\rSum(bbalance)\rFROM pgbench_history h,\rpgbench_branches b\rWHERE b.bid = h.bid\rAND b.bid IN ( 1, 2, 3 ); Disable manual capture of new SQL statements with their plans after you capture the execution plan for the desired SQL statement. QPM continues to capture new plans for managed SQL statements even after setting apg_plan_mgmt.capture_plan_baselines to off.\nSET apg_plan_mgmt.capture_plan_baselines = off; View captured query plan for the specific query that you ran previously. The plan_outline column in the table apg_plan_mgmt.dba_plans shows the entire plan for the SQL. For brevity, the plan_outline isn\u0026rsquo;t shown here. SELECT sql_hash,\rplan_hash,\rstatus,\restimated_total_cost \u0026#34;cost\u0026#34;,\rsql_text\rFROM apg_plan_mgmt.dba_plans; To instruct the query optimizer to use the approved or preferred captured plans for your managed statements, set the parameter apg_plan_mgmt.use_plan_baselines to true. SET apg_plan_mgmt.use_plan_baselines = true; View the explain plan output to see that the QPM approved plan is used by the query optimizer. explain (analyze, summary, hashes)\rSELECT Sum(delta),\rSum(bbalance)\rFROM pgbench_history h,\rpgbench_branches b\rWHERE b.bid = h.bid\rAND b.bid IN ( 1, 2, 3 ); Create a new index on the pgbench_history table column bid to change the planner configuration and force the query optimizer to generate a new plan. create index pgbench_hist_bid on pgbench_history(bid); View the explain plan output to see that QPM detects a new plan but still uses the approved plan and maintains the plan stability. Note the line An Approved plan was used instead of the minimum cost plan. in the explain plan output. explain (analyze, summary, hashes)\rSELECT Sum(delta),\rSum(bbalance)\rFROM pgbench_history h,\rpgbench_branches b\rWHERE b.bid = h.bid\rAND b.bid IN ( 1, 2, 3 ); Run the following SQL query to view the new plan and status of the plan. To ensure plan stability, QPM stores all the newly generated plans for a managed query in QPM as unapproved plans. The following output shows that there are two different execution plans stored for the same managed statement, as shown by the two different plan_hash values. Although the new execution plan has the minimum cost (lower than the approved plan), QPM continues to ignore the unapproved plans to maintain plan stability.\nThe plan_outline column in the table apg_plan_mgmt.dba_plans shows the entire plan for the SQL. For brevity, the plan_outline is not shown here.\nSELECT sql_hash,\rplan_hash,\rstatus,\restimated_total_cost \u0026#34;cost\u0026#34;,\rsql_text\rFROM apg_plan_mgmt.dba_plans; The following is an example of plan adaptability with QPM. This example evaluates the unapproved plan based on the minimum speedup factor. It approves any captured unapproved plan that is faster by at least 10 percent than the best approved plan for the statement. For additional details, see Evaluating Plan Performance in the Aurora documentation. SELECT apg_plan_mgmt.Evolve_plan_baselines (sql_hash, plan_hash, 1.1,\u0026#39;approve\u0026#39;)\rFROM apg_plan_mgmt.dba_plans\rWHERE status = \u0026#39;Unapproved\u0026#39;; After QPM evaluates the plan based on the speed factor, the plan status changes from unapproved to approved. At this point, the optimizer can choose the newly approved lower cost plan for that managed statement. SELECT sql_hash,\rplan_hash,\rstatus,\restimated_total_cost \u0026#34;cost\u0026#34;,\rsql_text\rFROM apg_plan_mgmt.dba_plans; View the explain plan output to see whether the query is using the newly approved minimum cost plan. explain (analyze, summary, hashes)\rSELECT Sum(delta),\rSum(bbalance)\rFROM pgbench_history h,\rpgbench_branches b\rWHERE b.bid = h.bid\rAND b.bid IN ( 1, 2, 3 ); "
},
{
	"uri": "//localhost:1313/5-clustercachemanagement/",
	"title": "Cluster Cache Management",
	"tags": [],
	"description": "",
	"content": "In this section, we will review Aurora Cluster Cache management (CCM) feature for Aurora PostgreSQL.\nFast Recovery After Failover with Cluster Cache Management for Aurora PostgreSQL In a typical failover situation, you might see a temporary but large performance degradation after failover. This degradation occurs because when the failover DB instance starts, the buffer cache is typically empty (Note that Amazon Aurora has a survivable cache feature which preserves the buffer cache after DB instance reboot). An empty cache is also known as a cold cache. A cold cache degrades performance because the DB instance has to read from the slower disk, instead of taking advantage of values stored in the buffer cache.\nWith cluster cache management, you set a specific reader DB instance as the failover target. Cluster cache management ensures that the data in the designated reader\u0026rsquo;s cache is kept synchronized with the data in the writer DB instance\u0026rsquo;s cache. The designated reader\u0026rsquo;s cache with prefilled values is known as a warm cache. If a failover occurs, the designated reader uses pages in its warm cache immediately when it\u0026rsquo;s promoted to the new writer DB instance. This approach provides your application much better recovery performance.\nIn the following sections, we will cover: Setup cluster cache management Benchmarking with Cluster Cache management "
},
{
	"uri": "//localhost:1313/2-preparation/2-5-createaupg/",
	"title": "Create Aurora PostgreSQL Database",
	"tags": [],
	"description": "",
	"content": "To create an Aurora PostgreSQL DB, follow these steps: Open the Amazon RDS console. Click Create database.\nFor Database engine, select PostgreSQL.\nFor Version, select the version of PostgreSQL that you want to use. For Template, select a template for your DB instance. A template is a pre-configured configuration for a DB instance. For Availability and Durability, For DB instance identifier, enter a name for your DB instance. For DB instance identifier, enter a unique name for your database instance.\nFor Master username, enter the username for the master user of your database instance.\nFor Master password, enter a strong password for the master user of your database instance. For Cluster storage configuration, select the amount of storage that you want to allocate for your database instance. For Instance configuration, select the DB instance class that you want to use for your database instance. The instance class will determine the amount of CPU and memory that is allocated to your database instance.\nFor Connectivity,\nLeave the Compute resource and Network type options at their Default values. Make sure the cluster Publicly accessible option is set to No. Leave Create an RDS Proxy option unchecked. For DB subnet group, select the DB subnet group that you want to use for your DB instance.\nFor VPC security groups, select the security groups that you want to use for your DB instance. Expand Additional configuration, Enter 5432 for database port\nLeave the Babelfish settings and Database authentication options at their default values. For Monitoring,\nCheck the box to Turn on Performance Insights with a Retention period for Performance Insights of 7 days (free tier) and use the (default) aws/rds AWS KMS key for monitoring data encryption.\nNext, expand the Additional configuration - Enhanced Monitoring section and check the Enable Enhanced Monitoring box, and select a Granularity of 1 second.\nExpand Additional configuration: Set the Initial database name to aupglab. Select the DB cluster parameter group with name aupg-parametergroup . For DB parameter group selectors, choose aupg-pg Choose a 7 days Backup retention period. Check the box to Enable encryption and select the (default) aws/rds for the Master key. For Log exports check the PostgreSQL log box. Leave the Maintenance options at their default values. Click Create database. It will take 5-10 minutes to create the Aurora cluster with a writer and a reader node.\nShow me summary of configuration options selected\rAurora PostgreSQL 15.3 compatible cluster on a db.r6g.large DB instance class Cluster composed of a writer and a reader DB instance in different availability zones (highly available) Uses Aurora Standard storage configuration in which I/Os are charged on a pay-per-request basis Deployed in the VPC in private subnets using the network configuration of the lab environment Attached with custom cluster and DB parameter groups Automatically backed up continuously, retaining backups for 7 days Using data at rest encryption With Enhanced Monitoring and Performance Insights enabled With PostgreSQL database log being exported to CloudWatch Created with an initial database called mylab With deletion protection turned off Store Aurora PostgreSQL credentials in AWS Secrets Manager What is the AWS Secrets Manager?\rAWS Secrets Manager helps you manage, retrieve, and rotate database credentials, application credentials, OAuth tokens, API keys, and other secrets throughout their lifecycles. Many AWS services store and use secrets in Secrets Manager.\nSecrets Manager helps you improve your security posture, because you no longer need hard-coded credentials in application source code. Storing the credentials in Secrets Manager helps avoid possible compromise by anyone who can inspect your application or the components. You replace hard-coded credentials with a runtime call to the Secrets Manager service to retrieve credentials dynamically when you need them.\nWith Secrets Manager, you can configure an automatic rotation schedule for your secrets. This enables you to replace long-term secrets with short-term ones, significantly reducing the risk of compromise. Since the credentials are no longer stored with the application, rotating credentials no longer requires updating your applications and deploying changes to application clients.\nFor other types of secrets you might have in your organization:\nAWS credentials – We recommend AWS Identity and Access Management.\nEncryption keys – We recommend AWS Key Management Service.\nSSH keys – We recommend Amazon EC2 Instance Connect.\nPrivate keys and certificates – We recommend AWS Certificate Manager.\nOpen the Secrets Manager console, then choose Store a new secret. On the Secret type, choose Credential for Amazon RDS database\nOn the Credential, input the User name (should be masteruser) and Password that you provided when you created the DB cluster previously Leave the Encryption Key options at their default values.\nOn the Database, choose the DB instance identifier you assigned to your instance (e.g. aupg-fcj-labs).\nClick Next. Name the secret aupg-fcj-labs-DBMateruser-secret and provide a relevant description for the secret, then click Next. Finally, in the Configure automatic rotation section, leave the option of Disable automatic rotation selected. In a production environment you will want to use database credentials that rotate automatically for additional security. Click Next.\nIn the Review section you have the ability to check the configuration parameters for your secret, before it gets created. Additionally, you can retrieve sample code in popular programming languages, so you can easily retrieve secrets into your application. Click Store at the bottom of the screen.\n"
},
{
	"uri": "//localhost:1313/2-preparation/2-6-configurecloud9andinitializedatabase/",
	"title": " Create, Configure Cloud9 and Initialize Database",
	"tags": [],
	"description": "",
	"content": "To create a Cloud9 enviroment, follow these steps: Open the Amazon Cloud9 console, then click Create enviroment Enter a name for your environment and select the Existing compute option for enviroment type. Under the Existing compute, click on button Copy key to clipboard Connect to EC2 instance via MobaXterm\nFollowing command below to save Public SSH key into file authorized_keys\ncd .ssh\rnano authorized_keys Install Nodejs with command below\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | bash\r. ~/.nvm/nvm.sh\rnvm install 16 Create folder cloud9_env\nmkdir cloud9_env Check path of Nodejs\nwhich node Install jq packages sudo yum install jq Go back step Create Cloud9 instance For User, enter EC2 user\nFor Host, enter your EC2 host\nUnder Additional details - optional For Environment path, type ~/cloud9_env For Path to Node.js binary, type the path that you have checked in previous step Then click Create\nAfter Successfully create Cloud9, click Open C9 install\nClick Next Click Next and leave everything ticked Click Finish Now, you have successfully created Cloud9 instance with existing compute\nConfigure the Cloud9 workstation Configure your AWS CLI with command below: aws configure Input your AWS Access Key ID Input your AWS Secret Access Key ID Input your Region name that you have handed-on lab Input Output format In the Cloud9 terminal window, paste and run the following commands.\nexport AWSREGION=`aws ec2 describe-availability-zones --output text --query \u0026#39;AvailabilityZones[0].[RegionName]\u0026#39;`\r# Replace your --db-cluster-identifier\rexport DBENDP=`aws rds describe-db-clusters --db-cluster-identifier aupg-fcj-labs --region $AWSREGION --query \u0026#39;DBClusters[*].Endpoint\u0026#39; | jq -r \u0026#39;.[0]\u0026#39;`\r# This assumes a \u0026#34;Name\u0026#34; tag was added to your secret with value aupg-fcj-labs-DBMasterUser-secret\rSECRETARN=`aws secretsmanager list-secrets --filters Key=\u0026#34;name\u0026#34;,Values=\u0026#34;aupg-fcj-labs-DBMasterUser-secret\u0026#34; --query \u0026#39;SecretList[*].ARN\u0026#39; | jq -r \u0026#39;.[0]\u0026#39;`\r# If below command doesnt show you the secret ARN, you should manually set the SECRETARN variable by referring it from the AWS Secrets manager console\recho $SECRETARN\rCREDS=`aws secretsmanager get-secret-value --secret-id $SECRETARN --region $AWSREGION | jq -r \u0026#39;.SecretString\u0026#39;`\rexport DBUSER=\u0026#34;`echo $CREDS | jq -r \u0026#39;.username\u0026#39;`\u0026#34;\rexport DBPASS=\u0026#34;`echo $CREDS | jq -r \u0026#39;.password\u0026#39;`\u0026#34;\recho DBENDP: $DBENDP\recho DBUSER: $DBUSER A notice disclaimerConfirm that you have output for the DBENDP and DBUSER variables. If not, you may not have named your Aurora cluster aupg-fcj-labs or tagged your Secret with Key:Name and Value: aupg-fcj-labs-DBMasterUser-secret. In that case, please set the DBENDP, DBUSER, and DBPASS variables manually in your terminal window before continuing.\nNow run these commands to save these variables to your local environment configuration file\nexport PGHOST=$DBENDP\rexport PGUSER=$DBUSER\rexport PGPASSWORD=\u0026#34;$DBPASS\u0026#34;\rexport PGDATABASE=aupglab\recho \u0026#34;export DBPASS=\\\u0026#34;$DBPASS\\\u0026#34;\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc\recho \u0026#34;export DBUSER=$DBUSER\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc\recho \u0026#34;export DBENDP=$DBENDP\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc\recho \u0026#34;export AWSREGION=$AWSREGION\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc\recho \u0026#34;export PGUSER=$DBUSER\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc\recho \u0026#34;export PGPASSWORD=\\\u0026#34;$DBPASS\\\u0026#34;\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc\recho \u0026#34;export PGHOST=$DBENDP\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc\recho \u0026#34;export PGDATABASE=aupglab\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc Now, you have saved the PostgreSQL specific environment variables in your Cloud9 Bash startup file which will make it convenient to login to Aurora PostgreSQL Cluster using psql.\nConnect, Verify and Initialize Database Instance Let\u0026rsquo;s make sure your Cloud9 environment and Aurora PostgreSQL database has been setup correctly.\nConnect to the aupglab database that was setup in the Aurora PostgreSQL cluster and verify the version of the database engine by running the following command in your Cloud9 terminal window.\npsql -c \u0026#39;select version(),AURORA_VERSION();\u0026#39; If you had setup your Cloud9 environment correctly, you should see output similar to the following:\nNow lets verify the user, database, host and port we are connecting to using the following command:\npsql\rselect current_user, current_database(), :\u0026#39;HOST\u0026#39; host, inet_server_port() port;\r\\q Since we are using the Aurora cluster endpoint to connect, we are connecting to the primary/writer DB instance of the Aurora PostgreSQL Cluster. Run the following commands in your Cloud9 terminal window to initialize the PostgreSQL database and make it ready for subsequent labs.\n# Ignore the ERROR messages below.\rpsql aupglab -f /home/ec2-user/clone_setup.sql \u0026gt; /home/ec2-user/clone_setup.output\rnohup pgbench -i --fillfactor=100 --scale=100 mylab \u0026amp;\u0026gt;\u0026gt; /tmp/nohup.out Ignore the table doesn\u0026rsquo;t exist error messages below: pgbench will take a minute or so to initialize the database. Once it completes, you are good to proceed to the next lab.\n"
},
{
	"uri": "//localhost:1313/6-databaseactivitystreaming/",
	"title": "Database activity streaming",
	"tags": [],
	"description": "",
	"content": "Monitoring your database activity can help you provide safeguards for your database and help to meet compliance and regulatory requirements. To monitor database activity with Amazon Aurora, you can use the Database Activity Streams feature. Database activity streams provide a near real-time data stream of the database activity in your relational database. When you integrate database activity streams with monitoring tools, you can monitor and audit database activity.\nContent Setup KMS for Database Activity Streaming Database Activity Streams in action "
},
{
	"uri": "//localhost:1313/7-rdsperformanceinsights/",
	"title": "RDS Performance Insights",
	"tags": [],
	"description": "",
	"content": "This lab will demonstrate the use of Amazon RDS Performance Insights . Amazon RDS Performance Insights (RDS PI) monitors your Amazon RDS DB instance load so that you can analyze and troubleshoot your database performance.\nThis lab contains the following tasks:\nLoad sample data to the Aurora PostgreSQL DB cluster Understand the RDS Performance Insights interface Use RDS Performance Insights to identify performance issue High volume insert load on the Aurora DB cluster using pgbench High volume update load on the Aurora DB cluster using pgbench Load sample data to the Aurora PostgreSQL DB cluster First, download all the required scripts used in this lab. Open a cloud9 terminal window by referring Open Cloud9 Terminal Window section and paste the commands below.\ncd\rwget wget https://aupg-fcj-assets.s3.us-west-2.amazonaws.com/lab-scripts/aupg-scripts.zip\runzip aupg-scripts.zip Create sample HR schema by running the following commands on the Cloud9 terminal window:\ncd /home/ec2-user/aupg-scripts/scripts\rpsql -f postgres-hr.sql # runs a PostgreSQL script named postgres-hr.sql Understanding the RDS Performance Insights interface While the command is running, open the Amazon RDS service console in a new tab, if not already open.\nNext, select the desired DB instance to load the performance metrics for. For Aurora DB clusters, performance metrics are exposed on an individual DB instance basis. The different DB instances comprising a cluster may run different workload patterns, and might not all have Performance Insights enabled. For this lab, we are generating load on the Writer (Primary) DB instance only.\nOnce a DB instance is selected, you will see the main dashboard view of RDS Performance Insights. The dashboard is divided into two sections, allowing you to drill down from high level performance indicator metrics down to individual waits, queries, users and hosts generating the load.\nThe performance metrics displayed by the dashboard are a moving time window. You can adjust the size of the time window by clicking the displayed time duration at the top right hand corner of the interface and selecting a relative range (5m, 1h, 5h, 24h, 1w, custom range) or specifying an absolute range. You can also zoom into a specific period of time by selecting with your mouse pointer and dragging across the graph\nAll dashboard views are time synchronized. Zooming in will adjust all views, including the detailed drill-down section at the bottom.\nHere is a summary of all the sections of RDS Performance Insights console.\nSection Filters Description Database load Load can be sliced by waits (default), application, database, hosts, session types, SQL commands and users This metric is designed to correlate aggregate load (sliced by the selected dimension) with the available compute capacity on that DB instance (number of vCPUs). Load is aggregated and normalized using the Average Active Session (AAS) metric. A number of AAS that exceeds the compute capacity of the DB instance is a leading indicator of performance problems. Granular Session Activity Sort by Waits, SQL (default), Hosts, Users, Session types, applications and databases Drill down capability that allows you to get detailed performance data down to the individual commands. Amazon Aurora PostgreSQL specific wait events are documented in the Amazon Aurora PostgreSQL Reference guide. Metrics Dashboard Click Metrics-new tab beside Dimensions to view counter metrics This section plots OS metrics, database metrics and CloudWatch metrics all in one place, such as number of rows read or written, transactions committed, etc. These metrics are useful to identify causes of abnormal behavior. This is how the new metrics dashboard looks like.\nUse RDS Performance Insights to identify performance issue In this exercise, we will learn how to use Performance Insights and PostgreSQL extensions to analyze the top wait events and performance issues. We will run some insert and update load test cases using pgbench utility on employees table in the HR schema.\nCreate pg_stat_statements extension\nIn a new psql session, connect to mylab database and run the following SQL command:\nBe sure to use a new psql session, otherwise your pg_stat_statements view will be created under the hr schema.\npsql\rCREATE EXTENSION pg_stat_statements;\r\\q Now, we are ready to run some load on the Aurora Instance to understand the capabilities of RDS Performance Insights.\nHigh volume insert load on the Aurora DB cluster using pgbench\nOn the cloud9 terminal window, run pgbench workload using the below command:\npgbench -n -c 10 -T 300 -f /home/ec2-user/aupg-scripts/scripts/hrload1.sql \u0026gt; /tmp/pgload1-run1.log The hrload1.sql SQL script will ingest employee records using PL/pgSQL function add_employee_data. This function uses employee_seq to generate the next employee_id, randomly generate data including first_name, salary with department_id from departments table. Each function call will insert 5 records. This test will be executed for 5 minutes with 10 clients.\nReview the PI dashboard and check the top wait events, AAS (Average Active Sessions) for the duration.\nYou will find below top 3 wait events:\nIO:XactSync - In this wait event, a session is issuing a COMMIT or ROLLBACK, requiring the current transaction’s changes to be persisted. Aurora is waiting for Aurora storage to acknowledge persistence.\nCPU\nLWLock:Buffer_content - In this wait event, a session is waiting to read or write a data page in memory while another session has that page locked for writing.\nNote down the key metrics in the pgbench output such as latency average and tps.\ncat /tmp/pgload1-run1.log Now, lets check the top 5 queries by execute time and CPU Consumption. Run the below SQL query to understand the load caused by the above pgbench run using pg_stat_statements extension.\npsql -c \u0026#34;SELECT substring(query, 1, 50) AS short_query, round(total_exec_time::numeric, 2) AS total_exec_time, calls, round(mean_exec_time::numeric, 2) AS mean_exec_time, round((100 * total_exec_time / sum(total_exec_time::numeric) OVER ())::numeric, 2) AS percentage_cpu FROM pg_stat_statements ORDER BY total_exec_time DESC LIMIT 5;\u0026#34; Explain psql command\r**psql -c**: Executes a SQL command directly from the command line.\\\rSELECT: Begins the SQL query.\nsubstring(query, 1, 50) AS short_query: Displays the first 50 characters of each query for brevity.\nround(total_exec_time::numeric, 2) AS total_exec_time: Shows the total execution time for each query, rounded to two decimal places.\ncalls: Indicates the number of times each query has been executed.\nround(mean_exec_time::numeric, 2) AS mean_exec_time: Shows the average execution time per call for each query.\nround((100 * total_exec_time / sum(total_exec_time::numeric) OVER ())::numeric, 2) AS percentage_cpu: Calculates the percentage of total CPU time consumed by each query.\nFROM pg_stat_statements: Accesses the pg_stat_statements view, which stores query execution statistics.\nORDER BY total_exec_time DESC: Sorts the results by total execution time in descending order (most resource-intensive queries first).\nLIMIT 5: Restricts the output to the top 5 results.\nLets rerun the same function with 50 inserts per execution and check the impact on wait events. Use hrload2.sql for this run.\npgbench -n -c 10 -T 300 -f /home/ec2-user/aurora-scripts/scripts/hrload2.sql \u0026gt; /tmp/pgload1-run2.log Go to PI dashboard and check the top wait events and top SQLs now and see if there are any changes. If you don\u0026rsquo;t see any new activity in the database load section, change the time range to last 5 minutes and click Apply. Then change it back to last 1 hour and click Apply.\nRerun the pg_stat_statements query to check resource consumption now.\npsql -c \u0026#34;SELECT substring(query, 1, 50) AS short_query, round(total_exec_time::numeric, 2) AS total_exec_time, calls, round(mean_exec_time::numeric, 2) AS mean_exec_time, round((100 * total_exec_time / sum(total_exec_time::numeric) OVER ())::numeric, 2) AS percentage_cpu FROM pg_stat_statements ORDER BY total_exec_time DESC LIMIT 5;\u0026#34; If you compare the wait events for the two pgbench runs, you will notice that IO:XactSync related waits have reduced in the latest run\nCan you verify whether the overall throughput (in terms of number of inserts) has increased by comparing the throughput and latencies reported by pgbench between the runs?\ncat /tmp/pgload1-run2.log High volume update load on the Aurora DB cluster using pgbench In this exercise, we will run updates on the employee table using update_employee_data_fname and update_employee_data_empid functions. On the cloud9 terminal window, run pgbench update workload using the below command:\npgbench -n -c 10 -T 180 -f /home/ec2-user/aurora-scripts/scripts/hrupdname.sql \u0026gt; /tmp/pgload2-run1.log The hrupdname.sql SQL script will update employee salary details in employees table using PL/pgSQL function update_employee_data_fname. This function randomly selects the employee records and checks if their salary is within a range (min and max salary of their job), if not updates their salary using their first_name. Each function call will select 5 records randomly. This test will be executed for 3 minutes with 10 clients.\nGo to RDS PI dashboard. Check the top wait events and AAS for the run duration.\nTop wait event is:\nCPU\nAlso check the CPU utilization Cloudwatch metrics for the Aurora cluster by selecting the Monitoring tab, searching for cpu and expanding the CPUUtilization graph.\nUpdate the graph to display 1 minute average. As you can see the CPUUtilization reached ~100% during the update load test.\nLet’s look at the performance stats using pg_stat_statements extensions.\nRun the below command and observe the top 5 queries consuming CPU.\npsql -c \u0026#34;SELECT substring(query, 1, 50) AS short_query, round(total_exec_time::numeric, 2) AS total_exec_time, calls, round(mean_exec_time::numeric, 2) AS mean_exec_time, round((100 * total_exec_time / sum(total_exec_time::numeric) OVER ())::numeric, 2) AS percentage_cpu FROM pg_stat_statements ORDER BY total_exec_time DESC LIMIT 5;\u0026#34; Let’s look at the explain plan used by the SQL statements in the PL/pgSQL function. In order to capture the explain plan in the logs, set the below DB parameters at your session level.\npsql\rset auto_explain.log_nested_statements=1;\rset auto_explain.log_min_duration=10; This will log any SQL statement including nested SQL statements which are taking more than 10ms in error/postgres.log with their corresponding explain plan.\nRun EXPLAIN ANALYZE to capture the explain plan as well as execute the query.\nEXPLAIN ANALYZE SELECT hr.update_employee_data_fname(10);\r\\q\r# hr.update_employee_data_fname(10): Calls the function update_employee_data_fname within the hr schema, passing the argument 10 Now, lets rerun the load using the SQL Script hrupdid.sql to use the employee_id column to update employees table.\nOn the cloud9 terminal window, run pgbench workload using the below command.\npgbench -n -c 10 -T 180 -f /home/ec2-user/aurora-scripts/scripts/hrupdid.sql \u0026gt; /tmp/pgload2-run2.log This will update employee salary details of employees using PL/pgSQL function update_employee_data_empid. This function randomly selects the employee records and checks if their salary is within a range (min and max salary of their job), if not updates their salary using their employee_id. Each function call will execute 5 records randomly. This test will be executed for 3 minutes with 10 clients.\nCompare the execution results using pg_stat_statements query again.\npsql -c \u0026#34;SELECT substring(query, 1, 50) AS short_query, round(total_exec_time::numeric, 2) AS total_exec_time, calls, round(mean_exec_time::numeric, 2) AS mean_exec_time, round((100 * total_exec_time / sum(total_exec_time::numeric) OVER ())::numeric, 2) AS percentage_cpu FROM pg_stat_statements ORDER BY total_exec_time DESC LIMIT 5;\u0026#34; Compare the throughput and latencies reported by pgbench between the runs.\ncat /tmp/pgload2-run1.log\rcat /tmp/pgload2-run2.log "
},
{
	"uri": "//localhost:1313/8-createdatasetandautoscale/",
	"title": "Create dataset and Auto Scale",
	"tags": [],
	"description": "",
	"content": "Aurora Auto Scaling enables your Aurora DB cluster to handle sudden increases in connectivity or workload by dynamically adjusting the number of Aurora Replicas for a provisioned Aurora DB cluster. When the connectivity or workload decreases, Aurora Auto Scaling removes unnecessary Aurora Replicas so that you don\u0026rsquo;t pay for unused DB instances.\nIn this lab, we will walk through how Aurora read replica auto scaling works in practice using a load generator script.\nThis lab contains the following tasks:\nConfigure aurora replica auto scaling Initialize pgbench and Create a Dataset Run a read-only workload Create a replica auto scaling policy You will add a read replica auto scaling configuration to the DB cluster. This will allow the DB cluster to scale the number of reader DB instances that operate in the DB cluster at any given point in time based on the load.\nClick on the Aurora cluster name and go to Logs \u0026amp; events tab. Click on the Add auto scaling policy button.\nEnter auroralab-autoscale-readers as the Policy Name. For the Target metric choose Average CPU utilization of Aurora Replicas. Enter a Target value of 20%. In a production use case this value may need to be set much higher, but we are using a lower value for demonstration purposes.\nNext, expand the Additional configuration section, and change both the Scale in cooldown period and Scale out cooldown period to a value of 180 seconds. This will reduce the time you have to wait between scaling operations in subsequent labs.\nIn the Cluster capacity details section, set the Minimum capacity to 1 and Maximum capacity to 2. In a production use case you may need to use different values, but for demonstration purposes, and to limit the cost associated with the labs we limit the number of readers to two.\nNext click Add policy.\nInitialize pgbench and Create a Dataset Open a Cloud9 terminal window by referring Open Cloud9 Terminal Window section and initialize pgbench to start the creation of dataset by pasting the command below in your terminal window.\npgbench -i --scale=1000 Data loading may take several minutes, you will receive similar output once complete: Run a read-only workload Once the data load completes successfully, you can run a read-only workload on the cluster (so that we can trigger our auto scaling policy). You will also observe the effects on the DB cluster topology.\nFor this step you will use the Reader Endpoint of the cluster. You can find the reader endpoint by going to the RDS Console - Databases section , clicking the name of the Aurora cluster and going to the Connectivity \u0026amp; security tab.\nRun the load generation script from your Cloud9 terminal window, replacing the [readerEndpoint] placeholder with the actual Aurora cluster reader endpoint:\npgbench -h [readerEndpoint] -c 100 --select-only -T 600 -C Now, open the Amazon RDS management console in a different browser tab.\nTake note that the reader node is currently receiving load. It may take a minute or more for the metrics to fully reflect the incoming load.\nAfter several minutes return to the list of instances and notice that a new reader is being provisioned in your cluster.\nIt will take 5-7 minutes to add a new replica. Once the new replica becomes available, note that the load distributes and stabilizes (it may take a few minutes to stabilize).\nYou can now toggle back to your Cloud9 terminal window, and press CTRL+C to quit the running pgbench job. After a while the additional reader will be removed automatically.\n"
},
{
	"uri": "//localhost:1313/9-testfaulttolerance/",
	"title": "Test Fault Tolerance",
	"tags": [],
	"description": "",
	"content": "An Aurora DB cluster is fault tolerant by design. The cluster volume spans 3 Availability Zones in a single AWS Region, and each Availability Zone contains 2 copies of the data. This functionality means that your DB cluster can tolerate a failure of an Availability Zone without any loss of data and only a brief interruption of service.\nIf the primary instance in a DB cluster using single-master replication fails, Aurora automatically fails over to a new primary instance in one of two ways:\nBy promoting an existing Aurora Replica to the new primary instance By creating a new primary instance\nIn this lab, we will simulate a failover of the Aurora PostgreSQL cluster, and observe the change in writer and reader roles. This lab contains the following tasks: Setup Failover event notifications Test a manual DB cluster failover Testing fault injection queries "
},
{
	"uri": "//localhost:1313/10-comparationrdspgandaupg/",
	"title": "Comparison RDS PostgreSQL and Aurora PostgreSQL",
	"tags": [],
	"description": "",
	"content": "\rWhat is AWS RDS PostgreSQL ?\rRDS PostgreSQL is a managed database service offered by Amazon Web Services (AWS) that makes it easy to set up, operate, and scale PostgreSQL deployments in the cloud. It handles many of the complex administrative tasks involved in managing a PostgreSQL database, allowing you to focus on developing and using your applications.\nHere\u0026rsquo;s how it works:\nDeployment: You choose the desired PostgreSQL version, instance size (compute and memory resources), storage type, and other configuration options. Provisioning: AWS handles the provisioning of the database instance, including installation, setup, and configuration. Management: RDS PostgreSQL automatically manages tasks like: Software patching and updates Backups and recovery Storage management Replication for high availability and read scaling Monitoring and performance tuning Access: You connect to your RDS PostgreSQL database using standard PostgreSQL clients and tools. Scaling: You can easily scale your database instance up or down as your needs change, without downtime. Key benefits of using RDS PostgreSQL:\nEase of use: Sets up in minutes with simple configuration options. Managed operations: Automates time-consuming administrative tasks. Cost-effectiveness: Offers pay-as-you-go pricing with no upfront costs. Scalability: Easily scales up or down to meet changing demands. High availability: Provides replication for failover and read scaling. Security: Secures data with encryption at rest and in transit. Compatibility: Works with standard PostgreSQL tools and applications. Common use cases for RDS PostgreSQL:\nWeb and mobile applications Data warehousing and analytics Enterprise resource planning (ERP) Customer relationship management (CRM) Content management systems (CMS) Internet of Things (IoT) applications What is AWS Aurora PostgreSQL ?\rAurora PostgreSQL, offered by Amazon Web Services (AWS), is a fully managed, highly scalable, and high-performance relational database service that\u0026rsquo;s fully compatible with PostgreSQL. It combines the best of both worlds: the simplicity and cost-effectiveness of open-source PostgreSQL with the speed, reliability, and advanced features of high-end commercial databases.\nHere\u0026rsquo;s a breakdown of its key features:\nScalability:\nScales virtually infinitely for both storage and compute capacity, unlike RDS PostgreSQL which has limitations. Automatically scales in increments of 10 GB for optimal performance. Read replicas are near real-time and minimize impact on the primary instance. Performance:\nUp to 5x faster than standard PostgreSQL, especially for read-heavy workloads. Offers low latency read replicas across multiple Availability Zones. Features like global database and cluster cache further boost performance. Durability and Availability:\nHighly durable with automatic backups and continuous replication. Automated failover to replicas in case of primary instance failure. Global database allows automatic failover across regions for disaster recovery. Other Features:\nServerless compute allows paying only for what you use. Up to 15 read replicas can be attached for increased read scalability. Integrates with other AWS services for simplified data management. Enhanced security features like encryption at rest and in transit. Performance: Aurora PostgreSQL: Up to 5x faster than traditional PostgreSQL and 3x faster than RDS PostgreSQL. Scales seamlessly without downtime. RDS PostgreSQL: Good performance for smaller workloads, but can struggle with high traffic or complex queries. Scaling requires downtime.\nBenchmarks Configure\nAurora PostgreSQL RDS PostgreSQL Instance type db.m1.lar5ge (2vCPU + 7.5Gb) db.m1.lar5ge (2vCPU + 7.5Gb) Region us-west-2a us-west-2a Client Side (running pgbench) EC2 instance in us-west-2a EC2 instance in us-west-2a Installed PG version 15.x 15.x Storage Encryption Enabled Enabled Multi-AZ/ Replication/ High-availability Disabled Disabled Benchmark details\nFollowing command below:\npgbench -c 10 -j 10 -t 500 -h [your endpoint] -U [your username] [dbname] Scalability: Aurora PostgreSQL: Scales automatically and continuously, without performance impact. Can handle massive datasets and millions of concurrent connections. RDS PostgreSQL: Requires manual scaling with limited options, leading to downtime and performance bottlenecks. Availability and Durability: Aurora PostgreSQL: Extremely high availability with automatic failover and multi-AZ backups. Provides point-in-time recovery up to the last five minutes. RDS PostgreSQL: Offers single-AZ deployments and manual backups. Failover requires configuration and potential data loss. Cost: Aurora PostgreSQL: Can be more expensive than RDS PostgreSQL, especially for low-traffic applications. However, cost savings can come from improved performance and reduced scaling needs. RDS PostgreSQL: Generally cheaper than Aurora PostgreSQL, but costs can quickly increase as you scale or require higher performance. Additional Factors: Features: Aurora PostgreSQL supports some features not available in RDS PostgreSQL, such as Babelfish for database migration and global databases. Compatibility: Both are compatible with PostgreSQL applications, but Aurora PostgreSQL has limitations on supported versions. Management: Both are fully managed services, but Aurora PostgreSQL handles more tasks automatically. Best practices Choose Aurora PostgreSQL for: high-traffic applications, scalability requirements, mission-critical databases, strict availability needs. Choose RDS PostgreSQL for: budget-sensitive applications, simple workloads, specific PostgreSQL features not available in Aurora, need for wider range of supported versions. "
},
{
	"uri": "//localhost:1313/11-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]