[
{
	"uri": "/vi/",
	"title": "AWS RDS PostgreSQL",
	"tags": [],
	"description": "",
	"content": "Let start with AWS RDS PostgreSQL Overall In this lab, you\u0026rsquo;ll learn the basics and practice of Amazon RDS PostgreSQL\nContent Introduction Start with PostgreSQL Create RDS PostgreSQL Database Instance Connect to pgAdmin4 Helpful Resources Clean up resources "
},
{
	"uri": "/vi/6-databaseactivitystreaming/6-1-setupkmsfordatabaseactivitystreamsinaction/",
	"title": "Configure the database client",
	"tags": [],
	"description": "",
	"content": "After Amazon RDS provisions your DB instance, you can use any standard SQL client application to connect to the instance. Before you can connect, the DB instance must be available and accessible. Whether you can connect to the instance from outside the VPC depends on how you created the Amazon RDS DB instance:\nIf you created your DB instance as public, devices and Amazon EC2 instances outside the VPC can connect to your database. If you created your DB instance as private, only Amazon EC2 instances and devices inside the Amazon VPC can connect to your database. title : \u0026ldquo;Setup KMS for Database Activity Streaming\u0026rdquo; date : \u0026ldquo;r Sys.Date()\u0026rdquo; weight : 1 chapter : false pre : \u0026quot; 6.1. \u0026quot; Database Activity Streaming requires a Master Key to encrypt the key that in turn encrypts the logged database activity. The Default AWS RDS KMS key can’t be used as the Master key. Therefore, we need to create a new customer managed KMS key to configure the Database Activity Streaming.\nCreate KMS Key Open KMS console and select Customer Managed Keys on the left-hand side and click on Create Key: On the next screen under Configure key choose Symmetric key type and click Next: On the next screen, under Add Labels give a name for the key under the field Alias such as cmk-apg-lab. Under Description field, type a description for the key such as Customer managed Key for Aurora PostgreSQL Database Activity Streaming (DAS) lab and click Next. On the next screen under Define Key Administrative permissions and Define key usage permissions, leave with default setting.\nOn the next screen, review the policy and click Finish.\nVerify the newly created KMS key on the KMS dashboard. In this chapter, we will use pgAdmin4 to connect to a RDS for PostgreSQL DB instance, so we need create DB as public.\nYou can use the open-source tool pgAdmin4 to connect to your RDS for PostgreSQL DB instance. You can download and install pgAdmin from http://www.pgadmin.org/ without having a local instance of PostgreSQL on your client computer\n1.Launch the pgAdmin4 application on your client computer.\n2.On the Dashboard tab, choose Add New Server. 3.In the Create - Server dialog box, type a name on the General tab to identify the server in pgAdmin4. 4.In the Connection tab, type the following information from your DB instance:\nFor Host, type the endpoint you have retrieve in the step 3.2, for example mypostgresql.c6c8dntfzzhgv0.us-east-2.rds.amazonaws.com. For Port, type the assigned port. For Username, type the user name that you entered when you created the DB instance. For Password, type the password that you entered when you created the DB instance. 5.Choose Save. If you have any problems connecting, see Troubleshooting connections to your RDS for PostgreSQL instance.\n"
},
{
	"uri": "/vi/2-preparation/2-1-createvpc/",
	"title": "Create a VPC",
	"tags": [],
	"description": "",
	"content": " Go to the VPC console and click Create VPC.\nFor Resources to create, choose VPC and more.\nEnter a name for your VPC and select a CIDR block. The CIDR block is the range of IP addresses that will be available to your VPC. Make sure to choose a CIDR block that is large enough for your needs, but not so large that you waste IP addresses.\nSelect values for Number of public subnets, Number of private subnets and NAT Gateway.\nReview your VPC resources, then click Create VPC\n"
},
{
	"uri": "/vi/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "\nAmazon Aurora PostgreSQL is a fully managed, PostgreSQL–compatible, and ACID–compliant relational database engine that combines the speed, reliability, and manageability of Amazon Aurora with the simplicity and cost-effectiveness of open-source databases. Aurora PostgreSQL is a drop-in replacement for PostgreSQL and makes it simple and cost-effective to set up, operate, and scale your new and existing PostgreSQL deployments, thus freeing you to focus on your business and applications. To learn more about Aurora in general, see What is Amazon Aurora?.\nIn addition to the benefits of Aurora, Aurora PostgreSQL offers a convenient migration pathway from Amazon RDS into Aurora, with push-button migration tools that convert your existing RDS for PostgreSQL applications to Aurora PostgreSQL. Routine database tasks such as provisioning, patching, backup, recovery, failure detection, and repair are also easy to manage with Aurora PostgreSQL.\nAurora PostgreSQL can work with many industry standards. For example, you can use Aurora PostgreSQL databases to build HIPAA-compliant applications and to store healthcare related information, including protected health information (PHI), under a completed Business Associate Agreement (BAA) with AWS.\nAurora PostgreSQL is FedRAMP HIGH eligible. For details about AWS and compliance efforts, see AWS services in scope by compliance program.\n"
},
{
	"uri": "/vi/2-preparation/",
	"title": "Preparation Steps",
	"tags": [],
	"description": "",
	"content": "The preparation steps using several components:\nAmazon VPC network configuration with public and private subnets Database subnet group and relevant security groups for the Aurora PostgreSQL cluster and AWS Cloud9 environment AWS Cloud9 configured with the software components needed for the labs Custom cluster and DB instance parameter groups for the Amazon Aurora PostgreSQL cluster, enabling some extensions and useful parameters The master database credentials will be store in AWS Secrets Manager Content Create VPC Create Security Group Create EC2 Create Aurora PostgreSQL Cluster Configure Cloud9 and Initialize Database "
},
{
	"uri": "/vi/9-testfaulttolerance/9-1-setupfailovereventnotifications/",
	"title": "Set up failover event notifications",
	"tags": [],
	"description": "",
	"content": "To receive notifications when failover events occur with your DB cluster, you will create an Amazon Simple Notification Service (SNS) topic, subscribe your email address to the SNS topic, create an RDS event subscription publishing events to the SNS topic and registering the DB cluster as an event source.\nOpen a Cloud9 terminal window by referring Open Cloud9 Terminal Window section and paste the following command to create an SNS topic.\naws sns create-topic \\\r--name auroralab-cluster-failovers If successful, the command will respond back with a TopicArn identifier, you will need this value in the next command.\nNext, subscribe your email address to the SNS topic using the command below, changing the placeholder [YourEmail] with your email address:\naws sns subscribe \\\r--topic-arn $(aws sns list-topics --query \u0026#39;Topics[?contains(TopicArn,`auroralab-cluster-failovers`)].TopicArn\u0026#39; --output text) \\\r--protocol email \\\r--notification-endpoint \u0026#39;[YourEmail]\u0026#39; You should see Output similar to the following:\nYou will receive a verification email on that address, please confirm the subscription by following the instructions in the email.\nOnce you click Confirm subscription in the email, you\u0026rsquo;ll see a browser window with a confirmation message as follows:\nOnce confirmed, or while you are waiting for the verification email to arrive, create an RDS event subscription and register the DB cluster as an event source using the command below:\nIf your Aurora cluster name is different than aupg-labs-cluster, update the command below accordingly.\naws rds create-event-subscription \\\r--subscription-name auroralab-cluster-failovers \\\r--sns-topic-arn $(aws sns list-topics --query \u0026#39;Topics[?contains(TopicArn,`auroralab-cluster-failovers`)].TopicArn\u0026#39; --output text) \\\r--source-type db-cluster \\\r--event-categories \u0026#39;[\u0026#34;failover\u0026#34;]\u0026#39; \\\r--enabled aws rds add-source-identifier-to-subscription \\\r--subscription-name auroralab-cluster-failovers \\\r--source-identifier aupg-fcj-labs At this time the event notifications have been configured. Ensure you have verified your email address before proceeding to the next section.\n"
},
{
	"uri": "/vi/5-clustercachemanagement/5-1-setupclustercachemanagement/",
	"title": "Setup cluster cache management",
	"tags": [],
	"description": "",
	"content": "Configuring Cluster Cache Management (CCM) Following are the steps to configure and enable the use of CCM on your Aurora PostgreSQL cluster\nModify the Amazon Aurora DB Cluster Parameters related to CCM. Sign in to the AWS Management Console and select Parameter Groups on the Amazon RDS console.\nIn the list, choose the DB cluster parameter group for your Aurora PostgreSQL DB cluster. The DB cluster must use a parameter group other than the default, because you can\u0026rsquo;t change values in a default parameter group. For more information, see Creating a DB Cluster Parameter Group.\nClick Edit under the Actions menu.\nSet the value of the apg_ccm_enabled cluster parameter to 1 and click on Save Changes.\nFor cluster cache management, make sure that the promotion priority is tier-0 for the writer DB instance of the Aurora PostgreSQL DB cluster. The promotion tier priority is a value that specifies the order in which an Aurora reader is promoted to the writer DB instance after a failure. Valid values are 0–15, where 0 is the first priority and 15 is the last priority. Select Databases in the Amazon RDS console.\nChoose the Writer DB instance of the Aurora PostgreSQL DB cluster and click on Modify\nThe Modify DB Instance page appears. Under Additional configuration, choose tier-0 for Failover Priority. Choose Continue and check the summary of modifications. To apply the changes immediately after you save them, choose Apply immediately and click Modify DB Instance to save your changes. For more information about setting the promotion tier, see Modify a DB Instance in a DB Cluster and the Promotion tier setting . See also Fault Tolerance for an Aurora DB Cluster. Next, set one reader DB instance for cluster cache management. To do so, choose a reader from the Aurora PostgreSQL cluster that is the same instance class and size as the writer DB instance. For example, if the writer uses db.r5.xlarge, choose a reader that uses this same instance class type and size. Then set its promotion tier priority to 0. The promotion tier priority is a value that specifies the order in which an Aurora replica is promoted to the primary DB instance after a failure. Valid values are 0 to 15, where 0 is the highest and 15 the lowest priority. In the navigation pane, choose Databases. Choose the Reader DB instance of the Aurora PostgreSQL DB cluster and click on Modify The Modify DB Instance page appears. Under Additional configuration, choose tier-0 for Failover Priority. Choose Continue and check the summary of modifications. To apply the changes immediately after you save them, choose Apply immediately and click Modify DB Instance to save your changes. Verifying if CCM is enabled Click on the DB identifier with the cluster name you created as a part of the CloudFormation stack or manually.\nUnder Connectivity and Security section, you will notice 2 different endpoints. The one with type Writer is the cluster endpoint (for read-write connections) and the one with type Reader is the reader endpoint (for read-only connections).\nOpen a cloud9 terminal window by referring Open Cloud9 Terminal Window section and using psql command line connect to the Aurora PostgreSQL DB cluster writer end point. Run the following SQL commands to check the cluster cache management status: psql \\x\rselect * from aurora_ccm_status(); If the Cluster Cache management is not enabled, querying aurora_ccm_status() will display the below output: aupglab=\u0026gt; \\x Expanded display is on. mylab=\u0026gt; select * from aurora_ccm_status(); ERROR: Cluster Cache Manager is disabled\n"
},
{
	"uri": "/vi/2-preparation/2-2-createsg/",
	"title": "Create Security Group ",
	"tags": [],
	"description": "",
	"content": "To create an EC2 security group in the AWS console: Go to the EC2 console. In the navigation pane, choose Security Groups. Choose Create Security Group. For VPC, choose the VPC where you want to create the security group. For Security group name, enter a descriptive name for the security group. For Description, enter a description for the security group. Modify Inbound Rule \u0026amp; Outbound Rule Click Create. Once you have created the security group for EC2 instances To create an Database security group in the AWS console: Go to the EC2 console. In the navigation pane, choose Security Groups. Choose Create Security Group. For VPC, choose the VPC where you want to create the security group. For Security group name, enter a descriptive name for the security group. For Description, enter a description for the security group. Modify Inbound Rule \u0026amp; Outbound Rule Click Create. Once you have created the security group for Aurora PostgreSQL "
},
{
	"uri": "/vi/5-clustercachemanagement/5-2-benchmarkingwithclustercachemanagement/",
	"title": "Database and Schemas",
	"tags": [],
	"description": "",
	"content": "Welcome to PostgreSQL! If this is your first time looking at PostgreSQL, we encourage you to check out the official About PostgreSQL webpage.\nIn this module, we are going to explore Databases and Schemas.\nThis chapter assumes you have setup and configured pgAdmin. If you haven\u0026rsquo;t, please complete the pgAdmin module before proceeding.\nExplore Databases 1.In your pgAdmin tool, click the \u0026gt; in front of rdspg-fcj-labs to expand it.\nYou see 3 breakouts: Databases, Login/Group Roles, and Tablespaces.\nIn this section, we will focus on Databases. And we\u0026rsquo;ll cover Login/Group Roles in a later section.\n2.Expand the Databases node.\nFrom a terminology standpoint, the PostgreSQL instance (rdspg-fcj-labs) you have created is known as a PostgreSQL cluster. A cluster contains one or more databases. While the users/roles of a cluster are shared across a cluster, no data is shared across databases. In other words, when a customer connects to a cluster, that connection is required to specify the database it wants to work with and that connection can only work within a single database at a time.\nyou see a handful of databases within the pglab cluster.\nWhat is the `rdsadmin` database ?\rThe database named rdsadmin is a database that is reserved for use by the RDS/Aurora control plane.\r3.Right-click on the Databases node and choose Create, then click Databases\n4.Name the database first_database (but don\u0026rsquo;t save it yet)\nThe database has an owner. This role can control and assign permissions for this database to other roles. The owner defaults to the role you are currently logged in with pgAdmin. Also note that you can alter the owner of most PostgreSQL objects even after they are originally created.\n5.Now click on the Definition tab\nThere are various settings that can be changed. You don\u0026rsquo;t need to change anything for now.\n6.Click on the SQL tab\nThe SQL tab shows you a preview of the generated SQL command that pgAdmin is going to run.\n7.Click Save 8.Find your new database in the navigator and expand it. Explore Schemas A database contains one or more named schemas, which in turn contain tables and other objects like views and functions. The objects in a given PostgreSQL schema can be owned by different users and the schema name has no implied correlation to the name of the schema owner.\nAs described in the PostgreSQL Documentation\n\u0026quot; The same object name can be used in different schemas without conflict; for example, both schema1 and myschema may contain tables named mytable. Unlike databases, schemas are not rigidly separated: a user may access objects in any of the schemas in the database he is connected to, if he has privileges to do so.\nThere are several reasons why one might want to use schemas:\nTo allow many users to use one database without interfering with each other. To organize database objects into logical groups to make them more manageable. Third-party applications can be put into separate schemas so they cannot collide with the names of other objects. Schemas are analogous to directories at the operating system level, except that schemas cannot be nested.\u0026quot;\n1.Expand the Schemas node and see a default public schema.\n2.Right-click on the Schemas node and choose Create, then click Schema.\n3.Name the schema first_schema\nThe schemas have an owner and also have security permissions and default privileges for new objects created in the schema (you can click on the Security tab and the Default Permissions tab in the Create Schema dialog if you want).\n4.Click Save to create the new schema.\nPostgreSQL schemas can be different from how other databases like Oracle implement schemas. In Oracle, schemas are directly mapped 1:1 to users. In PostgreSQL, schemas are not coupled directly to a specific user(role).\nAs discussed in the documentation ,\n\u0026ldquo;In the SQL standard, the notion of objects in the same schema being owned by different users does not exist. Moreover, some implementations do not allow you to create schemas that have a different name than their owner. In fact, the concepts of schema and user are nearly equivalent in a database system that implements only the basic schema support specified in the standard. Therefore, many users consider qualified names to really consist of username.tablename. This is how PostgreSQL will effectively behave if you create a per-user schema for every user. Also, there is no concept of a public schema in the SQL standard. For maximum conformance to the standard, you should not use (perhaps even remove) the public schema.\u0026rdquo;\nA note about the search_path Referencing objects via Qualified names, such as first_schema.first_table, is tedious to write, and hard-coding a particular schema name into an application is not ideal. The solution is to use unqualified names, such as first_table and this is made possible via the PostgreSQL search_path.\nAs discussed in the documentation ,\n\u0026ldquo;The system determines which table is meant by following a search_path, which is a list of schemas to look in. The first matching table in the search path is taken to be the one wanted. If there is no match in the search path, an error is reported, even if matching table names exist in other schemas in the database.\nThe first schema named in the search path is called the current schema. Aside from being the first schema searched, it is also the schema in which new tables will be created if the CREATE TABLE command does not specify a schema name.\u0026rdquo;\nBy default, the search_path is set to $user,public. As the documentation states\n\u0026ldquo;The first element specifies that a schema with the same name as the current user is to be searched. If no such schema exists, the entry is ignored. The second element refers to the public schema that we have seen already.\u0026rdquo;\nIt should be noted that PostgreSQL does not have the concept of synonyms like certain other databases. You can use the search_path to handle some, but not all, of the capabilities that synonyms offer. For an example of implementing other synonym-like functionality in PostgreSQL, see this blog post .\nCongratulations!\nYou have learned the basics about PostgreSQL Databases and Schemas.\u0026mdash; title : \u0026ldquo;Database and Schemas\u0026rdquo; date : \u0026ldquo;r Sys.Date()\u0026rdquo; weight : 2 chapter : false pre : \u0026quot; 2.2. \u0026quot; Welcome to PostgreSQL! If this is your first time looking at PostgreSQL, we encourage you to check out the official About PostgreSQL webpage.\nIn this module, we are going to explore Databases and Schemas.\nThis chapter assumes you have setup and configured pgAdmin. If you haven\u0026rsquo;t, please complete the pgAdmin module before proceeding.\nExplore Databases 1.In your pgAdmin tool, click the \u0026gt; in front of rdspg-fcj-labs to expand it.\nYou see 3 breakouts: Databases, Login/Group Roles, and Tablespaces.\nIn this section, we will focus on Databases. And we\u0026rsquo;ll cover Login/Group Roles in a later section.\n2.Expand the Databases node.\nFrom a terminology standpoint, the PostgreSQL instance (rdspg-fcj-labs) you have created is known as a PostgreSQL cluster. A cluster contains one or more databases. While the users/roles of a cluster are shared across a cluster, no data is shared across databases. In other words, when a customer connects to a cluster, that connection is required to specify the database it wants to work with and that connection can only work within a single database at a time.\nyou see a handful of databases within the pglab cluster.\nWhat is the `rdsadmin` database ?\rThe database named rdsadmin is a database that is reserved for use by the RDS/Aurora control plane.\r3.Right-click on the Databases node and choose Create, then click Databases\n4.Name the database first_database (but don\u0026rsquo;t save it yet)\nThe database has an owner. This role can control and assign permissions for this database to other roles. The owner defaults to the role you are currently logged in with pgAdmin. Also note that you can alter the owner of most PostgreSQL objects even after they are originally created.\n5.Now click on the Definition tab\nThere are various settings that can be changed. You don\u0026rsquo;t need to change anything for now.\n6.Click on the SQL tab\nThe SQL tab shows you a preview of the generated SQL command that pgAdmin is going to run.\n7.Click Save 8.Find your new database in the navigator and expand it. Explore Schemas A database contains one or more named schemas, which in turn contain tables and other objects like views and functions. The objects in a given PostgreSQL schema can be owned by different users and the schema name has no implied correlation to the name of the schema owner.\nAs described in the PostgreSQL Documentation\n\u0026quot; The same object name can be used in different schemas without conflict; for example, both schema1 and myschema may contain tables named mytable. Unlike databases, schemas are not rigidly separated: a user may access objects in any of the schemas in the database he is connected to, if he has privileges to do so.\nThere are several reasons why one might want to use schemas:\nTo allow many users to use one database without interfering with each other. To organize database objects into logical groups to make them more manageable. Third-party applications can be put into separate schemas so they cannot collide with the names of other objects. Schemas are analogous to directories at the operating system level, except that schemas cannot be nested.\u0026quot;\n1.Expand the Schemas node and see a default public schema.\n2.Right-click on the Schemas node and choose Create, then click Schema.\n3.Name the schema first_schema\nThe schemas have an owner and also have security permissions and default privileges for new objects created in the schema (you can click on the Security tab and the Default Permissions tab in the Create Schema dialog if you want).\n4.Click Save to create the new schema.\nPostgreSQL schemas can be different from how other databases like Oracle implement schemas. In Oracle, schemas are directly mapped 1:1 to users. In PostgreSQL, schemas are not coupled directly to a specific user(role).\nAs discussed in the documentation ,\n\u0026ldquo;In the SQL standard, the notion of objects in the same schema being owned by different users does not exist. Moreover, some implementations do not allow you to create schemas that have a different name than their owner. In fact, the concepts of schema and user are nearly equivalent in a database system that implements only the basic schema support specified in the standard. Therefore, many users consider qualified names to really consist of username.tablename. This is how PostgreSQL will effectively behave if you create a per-user schema for every user. Also, there is no concept of a public schema in the SQL standard. For maximum conformance to the standard, you should not use (perhaps even remove) the public schema.\u0026rdquo;\nA note about the search_path Referencing objects via Qualified names, such as first_schema.first_table, is tedious to write, and hard-coding a particular schema name into an application is not ideal. The solution is to use unqualified names, such as first_table and this is made possible via the PostgreSQL search_path.\nAs discussed in the documentation ,\n\u0026ldquo;The system determines which table is meant by following a search_path, which is a list of schemas to look in. The first matching table in the search path is taken to be the one wanted. If there is no match in the search path, an error is reported, even if matching table names exist in other schemas in the database.\nThe first schema named in the search path is called the current schema. Aside from being the first schema searched, it is also the schema in which new tables will be created if the CREATE TABLE command does not specify a schema name.\u0026rdquo;\nBy default, the search_path is set to $user,public. As the documentation states\n\u0026ldquo;The first element specifies that a schema with the same name as the current user is to be searched. If no such schema exists, the entry is ignored. The second element refers to the public schema that we have seen already.\u0026rdquo;\nIt should be noted that PostgreSQL does not have the concept of synonyms like certain other databases. You can use the search_path to handle some, but not all, of the capabilities that synonyms offer. For an example of implementing other synonym-like functionality in PostgreSQL, see this blog post .\nCongratulations!\nYou have learned the basics about PostgreSQL Databases and Schemas.\n"
},
{
	"uri": "/vi/3-fastcloning/",
	"title": "Start with PostgreSQL",
	"tags": [],
	"description": "",
	"content": "This lab contains following tasks: Content pgAdmin Database and Schemas Tables and Datatypes Basic DML Role and Users Procedural Code Catalog and Data dictionary Session parameters "
},
{
	"uri": "/vi/5-clustercachemanagement/",
	"title": "Start with PostgreSQL",
	"tags": [],
	"description": "",
	"content": "This lab contains following tasks: Content pgAdmin Database and Schemas Tables and Datatypes Basic DML Role and Users Procedural Code Catalog and Data dictionary Session parameters "
},
{
	"uri": "/vi/9-testfaulttolerance/9-2-testamanualdbclusterfailover/",
	"title": "Test a manual DB cluster failover",
	"tags": [],
	"description": "",
	"content": "In this test, we will use a Python script to connect to the cluster endpoint and continuously run a monitoring query.\nYou will need to open an additional Cloud9 terminal window as shown below. You will execute commands in one and see the results in the other session.\nDownload the failover test script with command below:\nwget https://aupg-fcj-assets.s3.us-west-2.amazonaws.com/lab-scripts/simple_failover.py In one of the two terminal windows, run the failover test script using the following command:\npython /home/ec2-user/simple_failover.py -e $DBENDP -u $DBUSER -p $DBPASS -d $PGDATABASE In the second cloud9 window, execute the command to initiate the failover\naws rds failover-db-cluster --db-cluster-identifier aupg-fcj-labs Initially the script would be connecting to the writer node and executing the query. You will see a slight pause and a message \u0026ldquo;waiting for failover\u0026rdquo; when the failover is initiated. Subsequently the time elapsed to re-connect and the new writer node information is printed.\nSince we are using the cluster endpoint for the connection, there is a slight delay to propagate the DNS record changes for the new writer node. You will see that for a few seconds after the failover, we are still connected to the old writer node which is now the new reader node. Once the DNS record change for the cluster endpoint is propagated to the client machine (in this case the Cloud9 workstation), the script will indicate that it is connected to the Writer node.\nYou will receive two event notification emails for each failover you initiate, one indicating that a failover has started, and one indicating that it has completed.\n"
},
{
	"uri": "/vi/6-databaseactivitystreaming/6-2-dbactivitystreamsinaction/",
	"title": "Verify DB instance",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/vi/2-preparation/2-3-createec2/",
	"title": "Create a EC2 instance",
	"tags": [],
	"description": "",
	"content": "To create an EC2 instance for an Cloud9 enviroment, follow these steps: Open the Amazon EC2 console. Click Launch Instance. Enter name your EC2 instance. Choose an AMI. For an app server, you can choose a Linux AMI, such as Amazon Linux 2. Choose an instance type. The instance type that you choose will depend on the requirements of your app server. For example, if you are running a high-traffic website, you will need a larger instance type with more CPU and memory. 5. For Key Pair,choose your keypair you have created or click Create new key pair\nConfigure the instance details. This includes things like the number of instances to launch, the network configuration, and the storage configuration.\nFor Network settings Choose VPC which contains EC2 app server Choose Subnet Enable Auto-assign public IP Add a security group for EC2 app server that you have created easier step . A security group is a firewall that controls incoming and outgoing traffic to your instance. Review and launch the instance Once the instance is launched, you can connect to it using an SSH client, such as MobaXterm or Putty. Once you are connected, you can install your app server and deploy your application.\n"
},
{
	"uri": "/vi/4-queryplanmanagement/",
	"title": "Create RDS PostgreSQL Database Instance",
	"tags": [],
	"description": "",
	"content": "This lab contains following tasks: Content Create DB instance Retrieve DB instance endpoint Store user/password database in AWS Secrets Manager secret "
},
{
	"uri": "/vi/9-testfaulttolerance/9-3-testingfaultinjectionqueries/",
	"title": "Testing fault injection queries",
	"tags": [],
	"description": "",
	"content": "In this test you will simulate a crash of the database engine service on the DB instance. This type of crash can be encountered in real circumstances as a result of out-of-memory conditions, or other unexpected circumstances.\nLearn more about fault injection queries\rFault injection queries provide a mechanism to simulate a variety of faults in the operation of the DB cluster. They are used to test the tolerance of client applications to such faults. They can be used to:\nSimulate crashes of the critical services running on the DB instance. These do not typically result in a failover to a reader, but will result in a restart of the relevant services. Simulate disk subsystem degradation or congestion, whether transient in nature or more persistent. Simulate read replica failures In one of the two terminal windows, run the failover test script using the following command:\npython /home/ec2-user/simple_failover.py -e $DBENDP -u $DBUSER -p $DBPASS -d $PGDATABASE Since we are using the cluster endpoint to connect, the motioning script is connected to the current writer node.\nOn the other Cloud9 terminal window, issue the following fault injection command. A crash of the PostgreSQL-compatible database for the Amazon Aurora instance will be simulated.\npsql -c \u0026#34;SELECT aurora_inject_crash (\u0026#39;instance\u0026#39;);\u0026#34; Wait and observe the monitoring script output. Once the crash is triggered, you should see an output similar to the example below.\nAs you see above, the instance was restarted and the monitoring script reconnected after a brief interruption.\n"
},
{
	"uri": "/vi/2-preparation/2-4-createpgsg/",
	"title": "Create Subnet Group and Parameter Group",
	"tags": [],
	"description": "",
	"content": "To create a DB subnet group on AWS, follow these steps: Open the Amazon RDS console In the navigation pane, choose Subnet groups and click on Create DB Subnet Group. For Name and Description, type in a name and description for your DB subnet group. For VPC, select the VPC in which you want to create your DB subnet group. Select the subnets that you want to include in your DB subnet group. Make sure to select subnets in at least two different Availability Zones (AZs). Click Create. Your DB subnet group will be created and will be displayed in the list of DB subnet groups.\nHere are some additional things to keep in mind when creating a DB subnet group:\nYou can only create a DB subnet group in a VPC that is in the same AWS Region as the database engine that you plan to use. You must include at least one subnet in each AZ in which you want to deploy your DB instance. You cannot modify a DB subnet group once it has been created. If you need to change the subnets in your DB subnet group, you must create a new one. You can use a DB subnet group to create a DB instance in any AZ in the VPC. To create a Database Parameter Group Go to the AWS RDS console, select Parameter Group, choose Create parameter group. In the Parameter group details section For Parameter group family, select aurora-postgresql15 For Type, select DB Parameter Group For Group Name, type your parameter group name For Description, type your description Click Create Change and enable some configurations in the parameter group Go to Parameter Group console, click Action, choose Edit Fill shared_preload_libraries parameter in the search bar, then select shared_preload_libraries Click Save Changes About shared_preload_libraries parameter The shared_preload_libraries parameter plays a crucial role in configuring your Aurora PostgreSQL instance. Purpose:\nThis parameter specifies which shared libraries are preloaded when your Aurora PostgreSQL server starts. Preloading libraries reduces the overhead of loading them on-demand when needed, potentially improving performance for specific functionalities.\nWhat libraries are preloaded?\nBuilt-in libraries: Certain core PostgreSQL functionalities rely on shared libraries that are automatically preloaded by default. You don\u0026rsquo;t need to configure them in shared_preload_libraries. Custom libraries: You can specify additional shared libraries to be preloaded for specific needs. These could be: PostgreSQL extensions: For features like full-text search or geospatial data handling. Custom modules: Developed by you or third-party vendors for unique functionalities. Important Notes:\nPerformance impact: Preloading unnecessary libraries can consume memory and negatively affect startup times. Only preload libraries actively used by your applications. Security considerations: Be cautious when adding custom libraries due to potential security vulnerabilities. Ensure they are from trusted sources and vetted carefully. Restart requirement: Modifying shared_preload_libraries requires restarting your Aurora PostgreSQL instance for the changes to take effect.\nTo create a Database Cluster Parameter Group Go to the AWS RDS console, select Parameter Group, choose Create parameter group. In the Parameter group details section For Parameter group family, select aurora-postgresql15 For Type, select DB Cluster Parameter Group For Group Name, type your parameter group name For Description, type your description Click Create Change and enable some configurations in the parameter group Go to Parameter Group console, click Action, choose Edit Fill log_rotation parameter in the search bar, Edit log_rotation_age value: 1440 Edit log_rotation_size value: 102400 Click Save Changes About log_rotation_age \u0026amp; log_rotation_size parameter\nlog_rotation_age\rFunction:\nlog_rotation_age defines the maximum age in minutes for individual log files within the cluster. Once a file reaches this age, it\u0026rsquo;s automatically rotated and a new one is created. This parameter helps manage disk space by preventing log files from growing indefinitely. Configuration:\nUnlike a standalone instance where you can set log_rotation_age in the postgresql.conf file or via command line, this parameter needs to be configured through a custom Aurora PostgreSQL parameter group. You can set the desired value within the parameter group and apply it to your cluster. Impact:\nSetting a shorter log_rotation_age value results in more frequent rotations and fresher log files, but can also increase disk I/O activity and potentially impact performance. A longer age reduces file rotations but retains older logs, which might not be necessary for your needs. Important Notes:\nRDS log rotation feature: While the RDS console lists log_rotation_age as a configurable parameter for its built-in log rotation feature, it currently has no effect on Aurora PostgreSQL clusters. You still need to use a custom parameter group to control file age rotation. CloudWatch Logs integration: Aurora PostgreSQL automatically streams logs to CloudWatch Logs by default. You can configure age-based retention policies within CloudWatch to manage the overall lifespan of log data, regardless of file rotations. Recommendations:\nChoose a log_rotation_age value that balances the need for disk space management with keeping sufficient log history for troubleshooting and analysis. Consider monitoring your cluster performance and adjusting the parameter value if necessary. Utilize CloudWatch Logs for long-term log retention and analysis beyond the file rotations managed by log_rotation_age.\nlog_rotation_size\rFunction:\nlog_rotation_size specifies the maximum size (in kilobytes) for an individual log file before it\u0026rsquo;s automatically rotated and a new one is created. This helps prevent excessive log growth and keeps the log directory manageable. Configuration:\nSimilar to log_rotation_age, you need to configure this parameter through a custom Aurora PostgreSQL parameter group. Set the desired size in kilobytes within the parameter group and apply it to your cluster. Impact:\nChoosing a smaller log_rotation_size value will trigger more frequent rotations, meaning smaller and fresher log files. However, this can increase disk I/O activity and impact performance slightly. Conversely, a larger size leads to fewer rotations and potentially larger logs, but may consume more disk space. Important Notes:\nAutomatic file naming: As files are rotated, Aurora PostgreSQL adds timestamps or sequence numbers to their filenames to maintain historical context. RDS log rotation feature: Similar to log_rotation_age, the RDS log rotation feature doesn\u0026rsquo;t currently control size-based rotation in Aurora PostgreSQL clusters. The custom parameter group approach is necessary. CloudWatch Logs integration: You can use CloudWatch Logs with its size-based retention policies to archive or delete rotated log files after exceeding a specific size. Recommendations:\nSelect a log_rotation_size value that balances disk space management with your log analysis needs. Consider the volume and size of your typical logs to estimate appropriate rotation intervals. Monitor your cluster performance and adjust the parameter value if necessary to avoid excessive rotations or large log files. Leverage CloudWatch Logs for long-term log retention and management, independent of rotations controlled by log_rotation_size. Remember, optimizing both log_rotation_age and log_rotation_size allows you to effectively manage your Aurora PostgreSQL cluster\u0026rsquo;s log files, ensuring sufficient data for analysis while limiting disk usage and potential performance impacts.\n"
},
{
	"uri": "/vi/6-databaseactivitystreaming/",
	"title": "Import data into PostgreSQl",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/vi/2-preparation/2-5-createaupg/",
	"title": "Create Aurora PostgreSQL Database",
	"tags": [],
	"description": "",
	"content": "To create an Aurora PostgreSQL DB, follow these steps: Open the Amazon RDS console. Click Create database. For Database engine, select PostgreSQL. For Version, select the version of PostgreSQL that you want to use. For Template, select a template for your DB instance. A template is a pre-configured configuration for a DB instance. For Availability and Durability, For DB instance identifier, enter a name for your DB instance. For DB instance identifier, enter a unique name for your database instance. For Master username, enter the username for the master user of your database instance. For Master password, enter a strong password for the master user of your database instance. For Cluster storage configuration, select the amount of storage that you want to allocate for your database instance. For Instance configuration, select the DB instance class that you want to use for your database instance. The instance class will determine the amount of CPU and memory that is allocated to your database instance. For Connectivity, Leave the Compute resource and Network type options at their Default values. Make sure the cluster Publicly accessible option is set to No. Leave Create an RDS Proxy option unchecked. For DB subnet group, select the DB subnet group that you want to use for your DB instance. For VPC security groups, select the security groups that you want to use for your DB instance. Expand Additional configuration, Enter 5432 for database port Leave the Babelfish settings and Database authentication options at their default values. For Monitoring, Check the box to Turn on Performance Insights with a Retention period for Performance Insights of 7 days (free tier) and use the (default) aws/rds AWS KMS key for monitoring data encryption. Next, expand the Additional configuration - Enhanced Monitoring section and check the Enable Enhanced Monitoring box, and select a Granularity of 1 second. 18. Expand Additional configuration:\nSet the Initial database name to aupglab. Select the DB cluster parameter group with name aupg-parametergroup . For DB parameter group selectors, choose aupg-pg Choose a 7 days Backup retention period. Check the box to Enable encryption and select the (default) aws/rds for the Master key. For Log exports check the PostgreSQL log box. Leave the Maintenance options at their default values. Click Create database. It will take 5-10 minutes to create the Aurora cluster with a writer and a reader node.\nShow me summary of configuration options selected\rAurora PostgreSQL 15.3 compatible cluster on a db.r6g.large DB instance class Cluster composed of a writer and a reader DB instance in different availability zones (highly available) Uses Aurora Standard storage configuration in which I/Os are charged on a pay-per-request basis Deployed in the VPC in private subnets using the network configuration of the lab environment Attached with custom cluster and DB parameter groups Automatically backed up continuously, retaining backups for 7 days Using data at rest encryption With Enhanced Monitoring and Performance Insights enabled With PostgreSQL database log being exported to CloudWatch Created with an initial database called mylab With deletion protection turned off Store Aurora PostgreSQL credentials in AWS Secrets Manager What is the AWS Secrets Manager?\rAWS Secrets Manager helps you manage, retrieve, and rotate database credentials, application credentials, OAuth tokens, API keys, and other secrets throughout their lifecycles. Many AWS services store and use secrets in Secrets Manager.\nSecrets Manager helps you improve your security posture, because you no longer need hard-coded credentials in application source code. Storing the credentials in Secrets Manager helps avoid possible compromise by anyone who can inspect your application or the components. You replace hard-coded credentials with a runtime call to the Secrets Manager service to retrieve credentials dynamically when you need them.\nWith Secrets Manager, you can configure an automatic rotation schedule for your secrets. This enables you to replace long-term secrets with short-term ones, significantly reducing the risk of compromise. Since the credentials are no longer stored with the application, rotating credentials no longer requires updating your applications and deploying changes to application clients.\nFor other types of secrets you might have in your organization:\nAWS credentials – We recommend AWS Identity and Access Management.\nEncryption keys – We recommend AWS Key Management Service.\nSSH keys – We recommend Amazon EC2 Instance Connect.\nPrivate keys and certificates – We recommend AWS Certificate Manager.\nOpen the Secrets Manager console, then choose Store a new secret. On the Secret type, choose Credential for Amazon RDS database On the Credential, input the User name (should be masteruser) and Password that you provided when you created the DB cluster previously Leave the Encryption Key options at their default values. On the Database, choose the DB instance identifier you assigned to your instance (e.g. aupg-fcj-labs). Click Next. Name the secret aupg-fcj-labs-DBMateruser-secret and provide a relevant description for the secret, then click Next. Finally, in the Configure automatic rotation section, leave the option of Disable automatic rotation selected. In a production environment you will want to use database credentials that rotate automatically for additional security. Click Next.\nIn the Review section you have the ability to check the configuration parameters for your secret, before it gets created. Additionally, you can retrieve sample code in popular programming languages, so you can easily retrieve secrets into your application. Click Store at the bottom of the screen.\n"
},
{
	"uri": "/vi/9-testfaulttolerance/",
	"title": "Querying data",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/vi/2-preparation/2-6-configurecloud9andinitializedatabase/",
	"title": " Create, Configure Cloud9 and Initialize Database",
	"tags": [],
	"description": "",
	"content": "To create a Cloud9 enviroment, follow these steps: Open the Amazon Cloud9 console, then click Create enviroment Enter a name for your environment and select the Existing compute option for enviroment type. Under the Existing compute, click on button Copy key to clipboard Connect to EC2 instance via MobaXterm Following command below to save Public SSH key into file authorized_keys cd .ssh\rnano authorized_keys Install Nodejs with command below curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | bash\r. ~/.nvm/nvm.sh\rnvm install 16 Create folder cloud9_env mkdir cloud9_env Check path of Nodejs which node Install jq packages sudo yum install jq Go back step Create Cloud9 instance For User, enter EC2 user For Host, enter your EC2 host Under Additional details - optional For Environment path, type ~/cloud9_env For Path to Node.js binary, type the path that you have checked in previous step Then click Create After Successfully create Cloud9, click Open C9 install Click Next Click Next and leave everything ticked Click Finish Now, you have successfully created Cloud9 instance with existing compute\nConfigure the Cloud9 workstation Configure your AWS CLI with command below: aws configure Input your AWS Access Key ID Input your AWS Secret Access Key ID Input your Region name that you have handed-on lab Input Output format In the Cloud9 terminal window, paste and run the following commands. export AWSREGION=`aws ec2 describe-availability-zones --output text --query \u0026#39;AvailabilityZones[0].[RegionName]\u0026#39;`\r# Replace your --db-cluster-identifier\rexport DBENDP=`aws rds describe-db-clusters --db-cluster-identifier aupg-fcj-labs --region $AWSREGION --query \u0026#39;DBClusters[*].Endpoint\u0026#39; | jq -r \u0026#39;.[0]\u0026#39;`\r# This assumes a \u0026#34;Name\u0026#34; tag was added to your secret with value aupg-fcj-labs-DBMasterUser-secret\rSECRETARN=`aws secretsmanager list-secrets --filters Key=\u0026#34;name\u0026#34;,Values=\u0026#34;aupg-fcj-labs-DBMasterUser-secret\u0026#34; --query \u0026#39;SecretList[*].ARN\u0026#39; | jq -r \u0026#39;.[0]\u0026#39;`\r# If below command doesnt show you the secret ARN, you should manually set the SECRETARN variable by referring it from the AWS Secrets manager console\recho $SECRETARN\rCREDS=`aws secretsmanager get-secret-value --secret-id $SECRETARN --region $AWSREGION | jq -r \u0026#39;.SecretString\u0026#39;`\rexport DBUSER=\u0026#34;`echo $CREDS | jq -r \u0026#39;.username\u0026#39;`\u0026#34;\rexport DBPASS=\u0026#34;`echo $CREDS | jq -r \u0026#39;.password\u0026#39;`\u0026#34;\recho DBENDP: $DBENDP\recho DBUSER: $DBUSER A notice disclaimerConfirm that you have output for the DBENDP and DBUSER variables. If not, you may not have named your Aurora cluster aupg-fcj-labs or tagged your Secret with Key:Name and Value: aupg-fcj-labs-DBMasterUser-secret. In that case, please set the DBENDP, DBUSER, and DBPASS variables manually in your terminal window before continuing.\nNow run these commands to save these variables to your local environment configuration file export PGHOST=$DBENDP\rexport PGUSER=$DBUSER\rexport PGPASSWORD=\u0026#34;$DBPASS\u0026#34;\rexport PGDATABASE=aupglab\recho \u0026#34;export DBPASS=\\\u0026#34;$DBPASS\\\u0026#34;\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc\recho \u0026#34;export DBUSER=$DBUSER\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc\recho \u0026#34;export DBENDP=$DBENDP\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc\recho \u0026#34;export AWSREGION=$AWSREGION\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc\recho \u0026#34;export PGUSER=$DBUSER\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc\recho \u0026#34;export PGPASSWORD=\\\u0026#34;$DBPASS\\\u0026#34;\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc\recho \u0026#34;export PGHOST=$DBENDP\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc\recho \u0026#34;export PGDATABASE=aupglab\u0026#34; \u0026gt;\u0026gt; /home/ec2-user/.bashrc Now, you have saved the PostgreSQL specific environment variables in your Cloud9 Bash startup file which will make it convenient to login to Aurora PostgreSQL Cluster using psql.\nConnect, Verify and Initialize Database Instance Let\u0026rsquo;s make sure your Cloud9 environment and Aurora PostgreSQL database has been setup correctly.\nConnect to the aupglab database that was setup in the Aurora PostgreSQL cluster and verify the version of the database engine by running the following command in your Cloud9 terminal window. psql -c \u0026#39;select version(),AURORA_VERSION();\u0026#39; If you had setup your Cloud9 environment correctly, you should see output similar to the following:\nNow lets verify the user, database, host and port we are connecting to using the following command: psql\rselect current_user, current_database(), :\u0026#39;HOST\u0026#39; host, inet_server_port() port;\r\\q Since we are using the Aurora cluster endpoint to connect, we are connecting to the primary/writer DB instance of the Aurora PostgreSQL Cluster. Run the following commands in your Cloud9 terminal window to initialize the PostgreSQL database and make it ready for subsequent labs. # Ignore the ERROR messages below.\rpsql aupglab -f /home/ec2-user/clone_setup.sql \u0026gt; /home/ec2-user/clone_setup.output\rnohup pgbench -i --fillfactor=100 --scale=100 mylab \u0026amp;\u0026gt;\u0026gt; /tmp/nohup.out Ignore the table doesn\u0026rsquo;t exist error messages below:\npgbench will take a minute or so to initialize the database. Once it completes, you are good to proceed to the next lab.\n"
},
{
	"uri": "/vi/11-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/vi/7-rdsperformanceinsights/",
	"title": "RDS Performance Insights",
	"tags": [],
	"description": "",
	"content": "This lab will demonstrate the use of Amazon RDS Performance Insights . Amazon RDS Performance Insights (RDS PI) monitors your Amazon RDS DB instance load so that you can analyze and troubleshoot your database performance.\nThis lab contains the following tasks:\nLoad sample data to the Aurora PostgreSQL DB cluster Understand the RDS Performance Insights interface Use RDS Performance Insights to identify performance issue High volume insert load on the Aurora DB cluster using pgbench High volume update load on the Aurora DB cluster using pgbench Load sample data to the Aurora PostgreSQL DB cluster First, download all the required scripts used in this lab. Open a cloud9 terminal window by referring Open Cloud9 Terminal Window section and paste the commands below.\ncd\rwget wget https://aupg-fcj-assets.s3.us-west-2.amazonaws.com/lab-scripts/aupg-scripts.zip\runzip aupg-scripts.zip Create sample HR schema by running the following commands on the Cloud9 terminal window:\ncd /home/ec2-user/aupg-scripts/scripts\rpsql -f postgres-hr.sql # runs a PostgreSQL script named postgres-hr.sql Understanding the RDS Performance Insights interface While the command is running, open the Amazon RDS service console in a new tab, if not already open.\nNext, select the desired DB instance to load the performance metrics for. For Aurora DB clusters, performance metrics are exposed on an individual DB instance basis. The different DB instances comprising a cluster may run different workload patterns, and might not all have Performance Insights enabled. For this lab, we are generating load on the Writer (Primary) DB instance only.\nOnce a DB instance is selected, you will see the main dashboard view of RDS Performance Insights. The dashboard is divided into two sections, allowing you to drill down from high level performance indicator metrics down to individual waits, queries, users and hosts generating the load.\nThe performance metrics displayed by the dashboard are a moving time window. You can adjust the size of the time window by clicking the displayed time duration at the top right hand corner of the interface and selecting a relative range (5m, 1h, 5h, 24h, 1w, custom range) or specifying an absolute range. You can also zoom into a specific period of time by selecting with your mouse pointer and dragging across the graph\nAll dashboard views are time synchronized. Zooming in will adjust all views, including the detailed drill-down section at the bottom.\nHere is a summary of all the sections of RDS Performance Insights console.\nSection Filters Description Database load Load can be sliced by waits (default), application, database, hosts, session types, SQL commands and users This metric is designed to correlate aggregate load (sliced by the selected dimension) with the available compute capacity on that DB instance (number of vCPUs). Load is aggregated and normalized using the Average Active Session (AAS) metric. A number of AAS that exceeds the compute capacity of the DB instance is a leading indicator of performance problems. Granular Session Activity Sort by Waits, SQL (default), Hosts, Users, Session types, applications and databases Drill down capability that allows you to get detailed performance data down to the individual commands. Amazon Aurora PostgreSQL specific wait events are documented in the Amazon Aurora PostgreSQL Reference guide. Metrics Dashboard Click Metrics-new tab beside Dimensions to view counter metrics This section plots OS metrics, database metrics and CloudWatch metrics all in one place, such as number of rows read or written, transactions committed, etc. These metrics are useful to identify causes of abnormal behavior. This is how the new metrics dashboard looks like.\nUse RDS Performance Insights to identify performance issue In this exercise, we will learn how to use Performance Insights and PostgreSQL extensions to analyze the top wait events and performance issues. We will run some insert and update load test cases using pgbench utility on employees table in the HR schema.\nCreate pg_stat_statements extension\nIn a new psql session, connect to mylab database and run the following SQL command:\nBe sure to use a new psql session, otherwise your pg_stat_statements view will be created under the hr schema.\npsql\rCREATE EXTENSION pg_stat_statements;\r\\q Now, we are ready to run some load on the Aurora Instance to understand the capabilities of RDS Performance Insights.\nHigh volume insert load on the Aurora DB cluster using pgbench\nOn the cloud9 terminal window, run pgbench workload using the below command:\npgbench -n -c 10 -T 300 -f /home/ec2-user/aupg-scripts/scripts/hrload1.sql \u0026gt; /tmp/pgload1-run1.log The hrload1.sql SQL script will ingest employee records using PL/pgSQL function add_employee_data. This function uses employee_seq to generate the next employee_id, randomly generate data including first_name, salary with department_id from departments table. Each function call will insert 5 records. This test will be executed for 5 minutes with 10 clients.\nReview the PI dashboard and check the top wait events, AAS (Average Active Sessions) for the duration.\nYou will find below top 3 wait events:\nIO:XactSync - In this wait event, a session is issuing a COMMIT or ROLLBACK, requiring the current transaction’s changes to be persisted. Aurora is waiting for Aurora storage to acknowledge persistence.\nCPU\nLWLock:Buffer_content - In this wait event, a session is waiting to read or write a data page in memory while another session has that page locked for writing.\nNote down the key metrics in the pgbench output such as latency average and tps.\ncat /tmp/pgload1-run1.log Now, lets check the top 5 queries by execute time and CPU Consumption. Run the below SQL query to understand the load caused by the above pgbench run using pg_stat_statements extension.\npsql -c \u0026#34;SELECT substring(query, 1, 50) AS short_query, round(total_exec_time::numeric, 2) AS total_exec_time, calls, round(mean_exec_time::numeric, 2) AS mean_exec_time, round((100 * total_exec_time / sum(total_exec_time::numeric) OVER ())::numeric, 2) AS percentage_cpu FROM pg_stat_statements ORDER BY total_exec_time DESC LIMIT 5;\u0026#34; Explain psql command\r**psql -c**: Executes a SQL command directly from the command line.\\\rSELECT: Begins the SQL query.\nsubstring(query, 1, 50) AS short_query: Displays the first 50 characters of each query for brevity.\nround(total_exec_time::numeric, 2) AS total_exec_time: Shows the total execution time for each query, rounded to two decimal places.\ncalls: Indicates the number of times each query has been executed.\nround(mean_exec_time::numeric, 2) AS mean_exec_time: Shows the average execution time per call for each query.\nround((100 * total_exec_time / sum(total_exec_time::numeric) OVER ())::numeric, 2) AS percentage_cpu: Calculates the percentage of total CPU time consumed by each query.\nFROM pg_stat_statements: Accesses the pg_stat_statements view, which stores query execution statistics.\nORDER BY total_exec_time DESC: Sorts the results by total execution time in descending order (most resource-intensive queries first).\nLIMIT 5: Restricts the output to the top 5 results.\nLets rerun the same function with 50 inserts per execution and check the impact on wait events. Use hrload2.sql for this run.\npgbench -n -c 10 -T 300 -f /home/ec2-user/aurora-scripts/scripts/hrload2.sql \u0026gt; /tmp/pgload1-run2.log Go to PI dashboard and check the top wait events and top SQLs now and see if there are any changes. If you don\u0026rsquo;t see any new activity in the database load section, change the time range to last 5 minutes and click Apply. Then change it back to last 1 hour and click Apply.\nRerun the pg_stat_statements query to check resource consumption now.\npsql -c \u0026#34;SELECT substring(query, 1, 50) AS short_query, round(total_exec_time::numeric, 2) AS total_exec_time, calls, round(mean_exec_time::numeric, 2) AS mean_exec_time, round((100 * total_exec_time / sum(total_exec_time::numeric) OVER ())::numeric, 2) AS percentage_cpu FROM pg_stat_statements ORDER BY total_exec_time DESC LIMIT 5;\u0026#34; If you compare the wait events for the two pgbench runs, you will notice that IO:XactSync related waits have reduced in the latest run\nCan you verify whether the overall throughput (in terms of number of inserts) has increased by comparing the throughput and latencies reported by pgbench between the runs?\ncat /tmp/pgload1-run2.log High volume update load on the Aurora DB cluster using pgbench In this exercise, we will run updates on the employee table using update_employee_data_fname and update_employee_data_empid functions. On the cloud9 terminal window, run pgbench update workload using the below command:\npgbench -n -c 10 -T 180 -f /home/ec2-user/aurora-scripts/scripts/hrupdname.sql \u0026gt; /tmp/pgload2-run1.log The hrupdname.sql SQL script will update employee salary details in employees table using PL/pgSQL function update_employee_data_fname. This function randomly selects the employee records and checks if their salary is within a range (min and max salary of their job), if not updates their salary using their first_name. Each function call will select 5 records randomly. This test will be executed for 3 minutes with 10 clients.\nGo to RDS PI dashboard. Check the top wait events and AAS for the run duration.\nTop wait event is:\nCPU\nAlso check the CPU utilization Cloudwatch metrics for the Aurora cluster by selecting the Monitoring tab, searching for cpu and expanding the CPUUtilization graph.\nUpdate the graph to display 1 minute average. As you can see the CPUUtilization reached ~100% during the update load test.\nLet’s look at the performance stats using pg_stat_statements extensions.\nRun the below command and observe the top 5 queries consuming CPU.\npsql -c \u0026#34;SELECT substring(query, 1, 50) AS short_query, round(total_exec_time::numeric, 2) AS total_exec_time, calls, round(mean_exec_time::numeric, 2) AS mean_exec_time, round((100 * total_exec_time / sum(total_exec_time::numeric) OVER ())::numeric, 2) AS percentage_cpu FROM pg_stat_statements ORDER BY total_exec_time DESC LIMIT 5;\u0026#34; Let’s look at the explain plan used by the SQL statements in the PL/pgSQL function. In order to capture the explain plan in the logs, set the below DB parameters at your session level.\npsql\rset auto_explain.log_nested_statements=1;\rset auto_explain.log_min_duration=10; This will log any SQL statement including nested SQL statements which are taking more than 10ms in error/postgres.log with their corresponding explain plan.\nRun EXPLAIN ANALYZE to capture the explain plan as well as execute the query.\nEXPLAIN ANALYZE SELECT hr.update_employee_data_fname(10);\r\\q\r# hr.update_employee_data_fname(10): Calls the function update_employee_data_fname within the hr schema, passing the argument 10 Now, lets rerun the load using the SQL Script hrupdid.sql to use the employee_id column to update employees table.\nOn the cloud9 terminal window, run pgbench workload using the below command.\npgbench -n -c 10 -T 180 -f /home/ec2-user/aurora-scripts/scripts/hrupdid.sql \u0026gt; /tmp/pgload2-run2.log This will update employee salary details of employees using PL/pgSQL function update_employee_data_empid. This function randomly selects the employee records and checks if their salary is within a range (min and max salary of their job), if not updates their salary using their employee_id. Each function call will execute 5 records randomly. This test will be executed for 3 minutes with 10 clients.\nCompare the execution results using pg_stat_statements query again.\npsql -c \u0026#34;SELECT substring(query, 1, 50) AS short_query, round(total_exec_time::numeric, 2) AS total_exec_time, calls, round(mean_exec_time::numeric, 2) AS mean_exec_time, round((100 * total_exec_time / sum(total_exec_time::numeric) OVER ())::numeric, 2) AS percentage_cpu FROM pg_stat_statements ORDER BY total_exec_time DESC LIMIT 5;\u0026#34; Compare the throughput and latencies reported by pgbench between the runs.\ncat /tmp/pgload2-run1.log\rcat /tmp/pgload2-run2.log "
},
{
	"uri": "/vi/8-createdatasetandautoscale/",
	"title": "Create dataset and Auto Scale",
	"tags": [],
	"description": "",
	"content": "Aurora Auto Scaling enables your Aurora DB cluster to handle sudden increases in connectivity or workload by dynamically adjusting the number of Aurora Replicas for a provisioned Aurora DB cluster. When the connectivity or workload decreases, Aurora Auto Scaling removes unnecessary Aurora Replicas so that you don\u0026rsquo;t pay for unused DB instances.\nIn this lab, we will walk through how Aurora read replica auto scaling works in practice using a load generator script.\nThis lab contains the following tasks:\nConfigure aurora replica auto scaling Initialize pgbench and Create a Dataset Run a read-only workload Create a replica auto scaling policy You will add a read replica auto scaling configuration to the DB cluster. This will allow the DB cluster to scale the number of reader DB instances that operate in the DB cluster at any given point in time based on the load.\nClick on the Aurora cluster name and go to Logs \u0026amp; events tab. Click on the Add auto scaling policy button.\nEnter auroralab-autoscale-readers as the Policy Name. For the Target metric choose Average CPU utilization of Aurora Replicas. Enter a Target value of 20%. In a production use case this value may need to be set much higher, but we are using a lower value for demonstration purposes.\nNext, expand the Additional configuration section, and change both the Scale in cooldown period and Scale out cooldown period to a value of 180 seconds. This will reduce the time you have to wait between scaling operations in subsequent labs.\nIn the Cluster capacity details section, set the Minimum capacity to 1 and Maximum capacity to 2. In a production use case you may need to use different values, but for demonstration purposes, and to limit the cost associated with the labs we limit the number of readers to two.\nNext click Add policy.\nInitialize pgbench and Create a Dataset Open a Cloud9 terminal window by referring Open Cloud9 Terminal Window section and initialize pgbench to start the creation of dataset by pasting the command below in your terminal window.\npgbench -i --scale=1000 Data loading may take several minutes, you will receive similar output once complete: Run a read-only workload Once the data load completes successfully, you can run a read-only workload on the cluster (so that we can trigger our auto scaling policy). You will also observe the effects on the DB cluster topology.\nFor this step you will use the Reader Endpoint of the cluster. You can find the reader endpoint by going to the RDS Console - Databases section , clicking the name of the Aurora cluster and going to the Connectivity \u0026amp; security tab.\nRun the load generation script from your Cloud9 terminal window, replacing the [readerEndpoint] placeholder with the actual Aurora cluster reader endpoint:\npgbench -h [readerEndpoint] -c 100 --select-only -T 600 -C Now, open the Amazon RDS management console in a different browser tab.\nTake note that the reader node is currently receiving load. It may take a minute or more for the metrics to fully reflect the incoming load.\nAfter several minutes return to the list of instances and notice that a new reader is being provisioned in your cluster.\nIt will take 5-7 minutes to add a new replica. Once the new replica becomes available, note that the load distributes and stabilizes (it may take a few minutes to stabilize).\nYou can now toggle back to your Cloud9 terminal window, and press CTRL+C to quit the running pgbench job. After a while the additional reader will be removed automatically.\n"
},
{
	"uri": "/vi/10-comparationrdspgandaupg/",
	"title": "Comparison RDS PostgreSQL and Aurora PostgreSQL",
	"tags": [],
	"description": "",
	"content": "\rWhat is AWS RDS PostgreSQL ?\rRDS PostgreSQL is a managed database service offered by Amazon Web Services (AWS) that makes it easy to set up, operate, and scale PostgreSQL deployments in the cloud. It handles many of the complex administrative tasks involved in managing a PostgreSQL database, allowing you to focus on developing and using your applications.\nHere\u0026rsquo;s how it works:\nDeployment: You choose the desired PostgreSQL version, instance size (compute and memory resources), storage type, and other configuration options. Provisioning: AWS handles the provisioning of the database instance, including installation, setup, and configuration. Management: RDS PostgreSQL automatically manages tasks like: Software patching and updates Backups and recovery Storage management Replication for high availability and read scaling Monitoring and performance tuning Access: You connect to your RDS PostgreSQL database using standard PostgreSQL clients and tools. Scaling: You can easily scale your database instance up or down as your needs change, without downtime. Key benefits of using RDS PostgreSQL:\nEase of use: Sets up in minutes with simple configuration options. Managed operations: Automates time-consuming administrative tasks. Cost-effectiveness: Offers pay-as-you-go pricing with no upfront costs. Scalability: Easily scales up or down to meet changing demands. High availability: Provides replication for failover and read scaling. Security: Secures data with encryption at rest and in transit. Compatibility: Works with standard PostgreSQL tools and applications. Common use cases for RDS PostgreSQL:\nWeb and mobile applications Data warehousing and analytics Enterprise resource planning (ERP) Customer relationship management (CRM) Content management systems (CMS) Internet of Things (IoT) applications What is AWS Aurora PostgreSQL ?\rAurora PostgreSQL, offered by Amazon Web Services (AWS), is a fully managed, highly scalable, and high-performance relational database service that\u0026rsquo;s fully compatible with PostgreSQL. It combines the best of both worlds: the simplicity and cost-effectiveness of open-source PostgreSQL with the speed, reliability, and advanced features of high-end commercial databases.\nHere\u0026rsquo;s a breakdown of its key features:\nScalability:\nScales virtually infinitely for both storage and compute capacity, unlike RDS PostgreSQL which has limitations. Automatically scales in increments of 10 GB for optimal performance. Read replicas are near real-time and minimize impact on the primary instance. Performance:\nUp to 5x faster than standard PostgreSQL, especially for read-heavy workloads. Offers low latency read replicas across multiple Availability Zones. Features like global database and cluster cache further boost performance. Durability and Availability:\nHighly durable with automatic backups and continuous replication. Automated failover to replicas in case of primary instance failure. Global database allows automatic failover across regions for disaster recovery. Other Features:\nServerless compute allows paying only for what you use. Up to 15 read replicas can be attached for increased read scalability. Integrates with other AWS services for simplified data management. Enhanced security features like encryption at rest and in transit. Performance: Aurora PostgreSQL: Up to 5x faster than traditional PostgreSQL and 3x faster than RDS PostgreSQL. Scales seamlessly without downtime. RDS PostgreSQL: Good performance for smaller workloads, but can struggle with high traffic or complex queries. Scaling requires downtime.\nBenchmarks Configure\nAurora PostgreSQL RDS PostgreSQL Instance type db.m1.lar5ge (2vCPU + 7.5Gb) db.m1.lar5ge (2vCPU + 7.5Gb) Region us-west-2a us-west-2a Client Side (running pgbench) EC2 instance in us-west-2a EC2 instance in us-west-2a Installed PG version 15.x 15.x Storage Encryption Enabled Enabled Multi-AZ/ Replication/ High-availability Disabled Disabled Benchmark details\nFollowing command below:\npgbench -c 10 -j 10 -t 500 -h [your endpoint] -U [your username] [dbname] Scalability: Aurora PostgreSQL: Scales automatically and continuously, without performance impact. Can handle massive datasets and millions of concurrent connections. RDS PostgreSQL: Requires manual scaling with limited options, leading to downtime and performance bottlenecks. Availability and Durability: Aurora PostgreSQL: Extremely high availability with automatic failover and multi-AZ backups. Provides point-in-time recovery up to the last five minutes. RDS PostgreSQL: Offers single-AZ deployments and manual backups. Failover requires configuration and potential data loss. Cost: Aurora PostgreSQL: Can be more expensive than RDS PostgreSQL, especially for low-traffic applications. However, cost savings can come from improved performance and reduced scaling needs. RDS PostgreSQL: Generally cheaper than Aurora PostgreSQL, but costs can quickly increase as you scale or require higher performance. Additional Factors: Features: Aurora PostgreSQL supports some features not available in RDS PostgreSQL, such as Babelfish for database migration and global databases. Compatibility: Both are compatible with PostgreSQL applications, but Aurora PostgreSQL has limitations on supported versions. Management: Both are fully managed services, but Aurora PostgreSQL handles more tasks automatically. Best practices Choose Aurora PostgreSQL for: high-traffic applications, scalability requirements, mission-critical databases, strict availability needs. Choose RDS PostgreSQL for: budget-sensitive applications, simple workloads, specific PostgreSQL features not available in Aurora, need for wider range of supported versions. "
},
{
	"uri": "/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]